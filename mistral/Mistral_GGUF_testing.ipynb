{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from C:\\Users\\Declan Bracken\\MEng_Project\\capybarahermes-2.5-mistral-7b.Q5_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = argilla_capybarahermes-2.5-mistral-7b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32002]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32002]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32002]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {% for message in messages %}{{'<|im_...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 261/32002 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32002\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.78 GiB (5.67 BPW) \n",
      "llm_load_print_meta: general.name     = argilla_capybarahermes-2.5-mistral-7b\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 32000 '<|im_end|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 32000 '<|im_end|>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4893.00 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 32768\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  4096.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 4096.00 MiB, K (f16): 2048.00 MiB, V (f16): 2048.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =  2144.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.name': 'argilla_capybarahermes-2.5-mistral-7b', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '32000', 'general.file_type': '17', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"}\n",
      "Guessed chat format: chatml\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "model_path = r\"C:\\Users\\Declan Bracken\\MEng_Project\\capybarahermes-2.5-mistral-7b.Q5_K_M.gguf\"\n",
    "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
    "llm = Llama(\n",
    "  model_path= model_path,  # Download the model file first\n",
    "  n_ctx=32768,  # The max sequence length to use - note that longer sequence lengths require much more resources\n",
    "  n_threads=8,            # The number of CPU threads to use, tailor to your system and the resulting performance\n",
    "  n_gpu_layers=-1         # The number of layers to offload to GPU, if you have GPU acceleration available\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Course Description Units Grade _— Points\n",
      "Course Description Units Grade Points\n",
      "Course Description Units Grade Points\n",
      "Course Description Units Grade _— Points\n",
      "Course Description Units Grade Points\n",
      "\n",
      "\n",
      "APSC 221 Economic And Business Practice 3.00 A 12.0\n",
      "ELEC 210 Intro Elec Circuits & Machines 4.00 A- 14.8\n",
      "MINE 244 Underground Mining 3.00 Bt 9.9\n",
      "MINE 267 App Chem/Instrument Meth Mine 4.00 A- 14.8\n",
      "MTHE 272 Applications Numerical Methods. 3.50 At 15.0\n",
      "MTHE 367 Engineering Data Analysis 4.00 B- 10.8\n",
      "APSC 100A Engineering Practice 0.00 NG 0.0\n",
      "APSC 111 Mechanics 3.50 A 14.0\n",
      "APSC 131 Chemistry And Materials 3.50 A 14.0\n",
      "APSC 151 Earth Systems And Engineering 4.00 At 17.2\n",
      "APSC 161 Engineering Graphics 3.50 A 14.0\n",
      "APSC 171 Calculus | 3.50 At 15.0\n",
      "APSC 200 Engr Design & Practice 4.00 At 17.2\n",
      "APSC 293 Engineering Communications 1.00 At 4.\n",
      "CIVL 230 Solid Mechanics | 4.25 At 18.\n",
      "MECH 230 Applied Thermodynamics | 3.50 At 15.\n",
      "MINE 201 Intro To Mining/Mineral Proces 4.00 A 16.(\n",
      "MINE 202 Comp Apps/Instrumntn In Mining 1.50 A 6.(\n",
      "MTHE 225 Ordinary Differential Equation 3.50 A 14.(\n",
      "APSC  100B Engineering Practice 11.00 A- 40.7\n",
      "APSC 112 Electricity And Magnetism 3.50 B+ 11.6\n",
      "APSC 132 Chemistry And The Environment 3.50 B 10.5\n",
      "APSC 142 Intro Computer Program Engrs 3.00 A- 11.1\n",
      "APSC 172 Calculus II 3.50 A- 13.0\n",
      "APSC 174 Introduction To Linear Algebra 3.50 At 15.0\n",
      "CLST 201 Roman History 3.00 At 12.¢\n",
      "ECON 111 Introductory Microeconomics 3.00 A- 11.1\n",
      "MINE 321 Drilling & Blasting 4.50 A- 16.6\n",
      "MINE 331 Methods Of Mineral Separation 4.50 A- 16.€\n",
      "MINE 339 Mine Ventilation 4.50 Ct 10.4\n",
      "MINE 341 Open Pit Mining 4.50 A- 16.6\n",
      "Academic Program History\n",
      "06/12/2012: Bachelor of Science Engineer Active in Program\n",
      "Major in General Engineering\n",
      "02/28/2013: Bachelor of Science Engineer Active in Program\n",
      "Major in Mining Engineering\n",
      "Option in Mining\n",
      "12/09/2014: Bachelor of Arts Active in Program\n",
      "Term GPA 3.51. Term Totals 24.00 24.00 84.3\n",
      "Term GPA 3.60 Term Totals 21.50 21.50 774\n",
      "Term GPA 4.18 Term Totals 21.75 21.75 90.8\n",
      "Term GPA 3.64 Term Totals 28.00 28.00 101.8\n",
      "Term GPA 4.13 Term Totals 18.00 18.00 74.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "APSC 221 Economic And Business Practice 3.00 A 12.0\n",
    "ELEC 210 Intro Elec Circuits & Machines 4.00 A- 14.8\n",
    "MINE 244 Underground Mining 3.00 Bt 9.9\n",
    "MINE 267 App Chem/Instrument Meth Mine 4.00 A- 14.8\n",
    "MTHE 272 Applications Numerical Methods. 3.50 At 15.0\n",
    "MTHE 367 Engineering Data Analysis 4.00 B- 10.8\n",
    "APSC 100A Engineering Practice 0.00 NG 0.0\n",
    "APSC 111 Mechanics 3.50 A 14.0\n",
    "APSC 131 Chemistry And Materials 3.50 A 14.0\n",
    "APSC 151 Earth Systems And Engineering 4.00 At 17.2\n",
    "APSC 161 Engineering Graphics 3.50 A 14.0\n",
    "APSC 171 Calculus | 3.50 At 15.0\n",
    "APSC 200 Engr Design & Practice 4.00 At 17.2\n",
    "APSC 293 Engineering Communications 1.00 At 4.\n",
    "CIVL 230 Solid Mechanics | 4.25 At 18.\n",
    "MECH 230 Applied Thermodynamics | 3.50 At 15.\n",
    "MINE 201 Intro To Mining/Mineral Proces 4.00 A 16.(\n",
    "MINE 202 Comp Apps/Instrumntn In Mining 1.50 A 6.(\n",
    "MTHE 225 Ordinary Differential Equation 3.50 A 14.(\n",
    "APSC  100B Engineering Practice 11.00 A- 40.7\n",
    "APSC 112 Electricity And Magnetism 3.50 B+ 11.6\n",
    "APSC 132 Chemistry And The Environment 3.50 B 10.5\n",
    "APSC 142 Intro Computer Program Engrs 3.00 A- 11.1\n",
    "APSC 172 Calculus II 3.50 A- 13.0\n",
    "APSC 174 Introduction To Linear Algebra 3.50 At 15.0\n",
    "CLST 201 Roman History 3.00 At 12.¢\n",
    "ECON 111 Introductory Microeconomics 3.00 A- 11.1\n",
    "MINE 321 Drilling & Blasting 4.50 A- 16.6\n",
    "MINE 331 Methods Of Mineral Separation 4.50 A- 16.€\n",
    "MINE 339 Mine Ventilation 4.50 Ct 10.4\n",
    "MINE 341 Open Pit Mining 4.50 A- 16.6\n",
    "Academic Program History\n",
    "06/12/2012: Bachelor of Science Engineer Active in Program\n",
    "Major in General Engineering\n",
    "02/28/2013: Bachelor of Science Engineer Active in Program\n",
    "Major in Mining Engineering\n",
    "Option in Mining\n",
    "12/09/2014: Bachelor of Arts Active in Program\n",
    "Term GPA 3.51. Term Totals 24.00 24.00 84.3\n",
    "Term GPA 3.60 Term Totals 21.50 21.50 774\n",
    "Term GPA 4.18 Term Totals 21.75 21.75 90.8\n",
    "Term GPA 3.64 Term Totals 28.00 28.00 101.8\n",
    "Term GPA 4.13 Term Totals 18.00 18.00 74.2\n",
    "\"\"\"\n",
    "\n",
    "headers = \"\"\"\n",
    "Course Description Units Grade _— Points\n",
    "Course Description Units Grade Points\n",
    "Course Description Units Grade Points\n",
    "Course Description Units Grade _— Points\n",
    "Course Description Units Grade Points\n",
    "\"\"\"\n",
    "print(headers + '\\n' + text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   69230.29 ms\n",
      "llama_print_timings:      sample time =      99.48 ms /   408 runs   (    0.24 ms per token,  4101.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =  157900.87 ms /  1148 tokens (  137.54 ms per token,     7.27 tokens per second)\n",
      "llama_print_timings:        eval time =  223717.18 ms /   407 runs   (  549.67 ms per token,     1.82 tokens per second)\n",
      "llama_print_timings:       total time =  383023.58 ms /  1555 tokens\n"
     ]
    }
   ],
   "source": [
    "prompt = f'''Below is OCR text from a student transcript. This text contains a table, or multiple tables. Select data only relevant to student courses and grades from these tables and format the fields into a table in csv format. Some extracted table headers are given below to help with formatting. The csv you output should only have 3 columns: 'Course Code', 'Grade', and 'Credits', you must select which columns best fit these fields.\n",
    "        \n",
    "        ### Headers:\n",
    "        {headers}\n",
    "\n",
    "        ### Text:\n",
    "        {text}\n",
    "\n",
    "        ### CSV:\n",
    "\n",
    "        '''\n",
    "\n",
    "system_message = \"You are a table creation assistant\"\n",
    "prompt_template=f'''<|im_start|>system\n",
    "{system_message}<|im_end|>\n",
    "<|im_start|>user\n",
    "{prompt}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "'''\n",
    "\n",
    "max_tokens = 2048\n",
    "temperature = 0\n",
    "top_p = 0\n",
    "echo = False\n",
    "stop = [\"</s>\"]\n",
    "\n",
    "\n",
    "# Define the parameters\n",
    "model_output = llm(\n",
    "       prompt_template,\n",
    "       max_tokens=max_tokens,\n",
    "       temperature=temperature,\n",
    "#        top_p=top_p,\n",
    "       echo=echo,\n",
    "       stop=stop,\n",
    "   )\n",
    "final_result = model_output[\"choices\"][0][\"text\"].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Course Code,Grade,Credits\n",
      "APSC 221,A,3.0\n",
      "ELEC 210,A-,4.0\n",
      "MINE 244,Bt,3.0\n",
      "MINE 267,A-,4.0\n",
      "MTHE 272,At,3.5\n",
      "MTHE 367,B-,4.0\n",
      "APSC 100A,NG,0.0\n",
      "APSC 111,A,3.5\n",
      "APSC 131,A,3.5\n",
      "APSC 151,At,4.0\n",
      "APSC 161,A,3.5\n",
      "APSC 171,At,3.5\n",
      "APSC 200,At,4.0\n",
      "CIVL 230,At,4.25\n",
      "MECH 230,At,3.5\n",
      "MINE 201,A,4.0\n",
      "MINE 202,A,1.5\n",
      "MTHE 225,A,3.5\n",
      "APSC  100B,A-,11.0\n",
      "APSC 112,B+,3.5\n",
      "APSC 132,B,3.5\n",
      "APSC 142,A-,3.0\n",
      "APSC 172,A-,3.5\n",
      "APSC 174,At,3.5\n",
      "CLST 201,At,3.0\n",
      "ECON 111,A-,3.0\n",
      "MINE 321,A-,4.5\n",
      "MINE 331,A-,4.5\n",
      "MINE 339,Ct,4.5\n",
      "MINE 341,A-,4.5\n"
     ]
    }
   ],
   "source": [
    "print(final_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
