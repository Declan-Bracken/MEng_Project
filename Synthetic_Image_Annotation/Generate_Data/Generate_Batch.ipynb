{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted and uploaded: SD_Trans_University_of_Chicago_-_Copy.jpeg (File ID: 1pI3toNgtVMDZ-HEKqGUqyr0z_6Tj46VC)\n",
      "Converted and uploaded: Sd-The-University-of-Alabama-Page-1.jpeg (File ID: 1LTl-rZAATpqczCM3Ene5C6tXsxVLIpxK)\n",
      "Converted and uploaded: SD_UTEPsn_-_Copy.jpeg (File ID: 1kSkSCW9j9yNISQJri3SB2_ifNhDr-6QF)\n",
      "Converted and uploaded: SD_Queens-College-Transcript-Page-1_-_Copy.jpeg (File ID: 12hqqW4o6-tLF9Tm4UDZmnZC5T-bHd7cO)\n",
      "Converted and uploaded: SD_Indiana_University_Trans_Pg_1_-_Copy.jpeg (File ID: 1a2SqGFdFGGX81zAHb9-9r7vi2r_SnkXb)\n",
      "Converted and uploaded: SD_Humber_College_Trans_Match_Sample.jpeg (File ID: 1BzQykPTXrYDo8qm45-Kqs5CwJy0i2kgL)\n",
      "Converted and uploaded: SD_Concordia_University-Page_1_-_Copy.jpeg (File ID: 1gDraQhEah7I9VX1FHSzm-Sinj_HBQhNs)\n",
      "Converted and uploaded: SD_Concordia_Transcript_HOrizontal_1_-_Copy.jpeg (File ID: 1T5P769wOOhsdY4sQ17tUFag7ReKyj3Zb)\n",
      "Converted and uploaded: SD_Pennsylvania-State-University.jpeg (File ID: 1K1eTxzaYPiP68_GS_AXOTm1nJ9BEoniq)\n",
      "Converted and uploaded: Stamford_Bangladesh_Uni.jpeg (File ID: 1qXufiu8cLDN5CCxRI69T867qZSsxOd2X)\n",
      "Converted and uploaded: Loughborough_Academy.jpeg (File ID: 1kqW27F0vtWl99s-u0XOYHMaOyya6ivsK)\n",
      "Converted and uploaded: Altiora.jpeg (File ID: 1WrHrCzvwxk5IyXpj2LPNBXqxkCl5Hmql)\n",
      "Converted and uploaded: Elon_Academy.jpeg (File ID: 1vKUpnIE73KoQslO-7cw_XhR4areynUP0)\n",
      "Converted and uploaded: Queens_College.jpeg (File ID: 1jtQjFKztmz2l5DAk1KPRq9YtYNayU4V6)\n",
      "Converted and uploaded: UNSW_Austrlia.jpeg (File ID: 1P8LvUcCM1NoIU2qo_cS4hCzRgfdFb_nL)\n",
      "Converted and uploaded: download.jpeg (File ID: 1poCyiTUbBH5uarIOtECYKyC2SBbxUOOY)\n"
     ]
    }
   ],
   "source": [
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from google.auth.transport.requests import Request\n",
    "from googleapiclient.http import MediaIoBaseUpload\n",
    "from PIL import Image\n",
    "import os\n",
    "import io\n",
    "\n",
    "class GoogleDriveManager:\n",
    "    def __init__(self, credentials_file='/Users/declanbracken/Development/UofT_Projects/Meng_Project/code_base/web_scraping/client_secret_809384080547-4sfr7l9u8a618keak7b11qan4o63nvh3.apps.googleusercontent.com.json', token_file='token.json', scopes=None):\n",
    "        self.credentials_file = credentials_file\n",
    "        self.token_file = token_file\n",
    "        self.scopes = ['https://www.googleapis.com/auth/drive']\n",
    "        self.service = self.authenticate_google_drive()\n",
    "\n",
    "    def authenticate_google_drive(self):\n",
    "        \"\"\"Authenticate and return a Google Drive service object.\"\"\"\n",
    "        creds = None\n",
    "        if os.path.exists(self.token_file):\n",
    "            creds = Credentials.from_authorized_user_file(self.token_file, self.scopes)\n",
    "        if not creds or not creds.valid:\n",
    "            if creds and creds.expired and creds.refresh_token:\n",
    "                creds.refresh(Request())\n",
    "            else:\n",
    "                flow = InstalledAppFlow.from_client_secrets_file(self.credentials_file, self.scopes)\n",
    "                creds = flow.run_local_server(port=0)\n",
    "            with open(self.token_file, 'w') as token:\n",
    "                token.write(creds.to_json())\n",
    "        return build('drive', 'v3', credentials=creds)\n",
    "    \n",
    "    def list_files_in_folder(self, folder_id):\n",
    "        query = f\"'{folder_id}' in parents and mimeType contains 'image/'\"\n",
    "        results = self.service.files().list(q=query, pageSize=1000, fields=\"nextPageToken, files(id, name)\").execute()\n",
    "        items = results.get('files', [])\n",
    "        return items\n",
    "\n",
    "    def get_file_urls(self, files):\n",
    "        file_urls = []\n",
    "        for file in files:\n",
    "            file_id = file['id']\n",
    "            file_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "            file_urls.append(file_url)\n",
    "        return file_urls\n",
    "\n",
    "    def convert_images_to_jpeg_and_upload(self, input_folder_id, output_folder_id):\n",
    "        \"\"\"\n",
    "        Convert all images in the input Google Drive folder to JPEG format and upload them to the output folder.\n",
    "        Necessary for gpt-4o inference if there are .jpg files or other formats which are unsupported.\n",
    "\n",
    "        Args:\n",
    "        - input_folder_id (str): The ID of the Google Drive folder containing the original images.\n",
    "        - output_folder_id (str): The ID of the Google Drive folder where the JPEG images will be uploaded.\n",
    "        \"\"\"\n",
    "        # Retrieve image files from the input folder\n",
    "        items = self.list_files_in_folder(input_folder_id)\n",
    "\n",
    "        if not items:\n",
    "            print(\"No image files found in the specified folder.\")\n",
    "            return\n",
    "\n",
    "        for item in items:\n",
    "            file_id = item['id']\n",
    "            file_name = item['name']\n",
    "\n",
    "            # Download the file content\n",
    "            request = self.service.files().get_media(fileId=file_id)\n",
    "            file_data = io.BytesIO(request.execute())\n",
    "\n",
    "            try:\n",
    "                # Open the image and convert it to JPEG\n",
    "                with Image.open(file_data) as img:\n",
    "                    img = img.convert(\"RGB\")  # Ensure the image is in RGB mode for JPEG\n",
    "\n",
    "                    # Prepare JPEG image in memory\n",
    "                    jpeg_io = io.BytesIO()\n",
    "                    jpeg_name = os.path.splitext(file_name)[0] + \".jpeg\"\n",
    "                    img.save(jpeg_io, \"JPEG\")\n",
    "                    jpeg_io.seek(0)\n",
    "\n",
    "                    # Upload the JPEG image back to Google Drive\n",
    "                    media = MediaIoBaseUpload(jpeg_io, mimetype='image/jpeg')\n",
    "                    file_metadata = {\n",
    "                        'name': jpeg_name,\n",
    "                        'parents': [output_folder_id]\n",
    "                    }\n",
    "                    uploaded_file = self.service.files().create(body=file_metadata, media_body=media, fields='id').execute()\n",
    "                    print(f\"Converted and uploaded: {jpeg_name} (File ID: {uploaded_file.get('id')})\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error converting {file_name}: {e}\")\n",
    "\n",
    "# Path to your service account key file\n",
    "SERVICE_ACCOUNT_FILE = '/Users/declanbracken/Development/UofT_Projects/Meng_Project/code_base/web_scraping/client_secret_809384080547-4sfr7l9u8a618keak7b11qan4o63nvh3.apps.googleusercontent.com.json'\n",
    "\n",
    "# ID of the folder containing the images\n",
    "# FOLDER_ID = '14zyq0BXTYrYj81bGlKtYpEG-KL59oNnM'\n",
    "IN_FOLDER_ID = '1KInbF1MOyQ-RSLWYcnLIoaCo3D9ieuOL'\n",
    "OUT_FOLDER_ID = '1qRtXRctr7jXr5cpxKiI_vxw0Bj-Ryp07'\n",
    "\n",
    "# Create manager instace\n",
    "manager = GoogleDriveManager(credentials_file=SERVICE_ACCOUNT_FILE)#, token_file=TOKEN_FILE)\n",
    "\n",
    "# Convert images in the input folder to JPEG and upload them to the output folder\n",
    "manager.convert_images_to_jpeg_and_upload(IN_FOLDER_ID, OUT_FOLDER_ID)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get folder items\n",
    "folder_items = manager.list_files_in_folder(OUT_FOLDER_ID)\n",
    "\n",
    "# List urls\n",
    "image_urls = manager.get_file_urls(folder_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://drive.google.com/uc?id=1poCyiTUbBH5uarIOtECYKyC2SBbxUOOY\n",
      "https://drive.google.com/uc?id=1P8LvUcCM1NoIU2qo_cS4hCzRgfdFb_nL\n",
      "https://drive.google.com/uc?id=1jtQjFKztmz2l5DAk1KPRq9YtYNayU4V6\n",
      "https://drive.google.com/uc?id=1vKUpnIE73KoQslO-7cw_XhR4areynUP0\n",
      "https://drive.google.com/uc?id=1WrHrCzvwxk5IyXpj2LPNBXqxkCl5Hmql\n",
      "https://drive.google.com/uc?id=1kqW27F0vtWl99s-u0XOYHMaOyya6ivsK\n",
      "https://drive.google.com/uc?id=1qXufiu8cLDN5CCxRI69T867qZSsxOd2X\n",
      "https://drive.google.com/uc?id=1K1eTxzaYPiP68_GS_AXOTm1nJ9BEoniq\n",
      "https://drive.google.com/uc?id=1T5P769wOOhsdY4sQ17tUFag7ReKyj3Zb\n",
      "https://drive.google.com/uc?id=1gDraQhEah7I9VX1FHSzm-Sinj_HBQhNs\n",
      "https://drive.google.com/uc?id=1BzQykPTXrYDo8qm45-Kqs5CwJy0i2kgL\n",
      "https://drive.google.com/uc?id=1a2SqGFdFGGX81zAHb9-9r7vi2r_SnkXb\n",
      "https://drive.google.com/uc?id=12hqqW4o6-tLF9Tm4UDZmnZC5T-bHd7cO\n",
      "https://drive.google.com/uc?id=1kSkSCW9j9yNISQJri3SB2_ifNhDr-6QF\n",
      "https://drive.google.com/uc?id=1LTl-rZAATpqczCM3Ene5C6tXsxVLIpxK\n",
      "https://drive.google.com/uc?id=1pI3toNgtVMDZ-HEKqGUqyr0z_6Tj46VC\n"
     ]
    }
   ],
   "source": [
    "# Print or use the list of image URLs\n",
    "for url in image_urls:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input.jsonl file created.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "def write_to_jsonl(prompts, image_urls, jsonl_path, max_tokens = 1200):\n",
    "    # Ensure the directory exists\n",
    "    # os.makedirs(os.path.dirname(jsonl_path), exist_ok=True)\n",
    "\n",
    "    # Generate JSONL content\n",
    "    jsonl_content = []\n",
    "    for i, url in enumerate(image_urls):\n",
    "        prompt = random.choice(prompts)\n",
    "        request = {\n",
    "            \"custom_id\": f\"request-{i+1}\",\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": \"gpt-4o\",\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\n",
    "                                \"type\": \"text\",\n",
    "                                \"text\": f\"{prompt}\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"type\": \"image_url\",\n",
    "                                \"image_url\": {\n",
    "                                    \"url\": f\"{url}\"\n",
    "                                }\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                ],\n",
    "                \"max_tokens\": max_tokens\n",
    "            }\n",
    "        }\n",
    "        jsonl_content.append(request)\n",
    "\n",
    "    # Write to JSONL file\n",
    "    with open(jsonl_path, \"w\") as f:\n",
    "        for entry in jsonl_content:\n",
    "            f.write(json.dumps(entry) + \"\\n\")\n",
    "    try:\n",
    "        print(f\"{jsonl_path.split('/')[-1]} file created.\")\n",
    "    except:\n",
    "        print(\"jsonl file created.\")\n",
    "\n",
    "# Define the prompts for different parts of the extraction\n",
    "prompts = [\n",
    "    \"Extract only the course information from the attached transcript image, including but not limited to subjects/course codes, grades, credits, and other information. Please then structure this information into a table in CSV format. If the image is not a transcript, respond with 'not a transcript'.\",\n",
    "]\n",
    "# JSONL path\n",
    "jsonl_path = 'Test_Data/Input_JSONL/input.jsonl'\n",
    "\n",
    "write_to_jsonl(prompts, image_urls, jsonl_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batchinput_part_1.jsonl file created with 8 lines.\n",
      "batchinput_part_2.jsonl file created with 8 lines.\n",
      "Total lines: 16\n"
     ]
    }
   ],
   "source": [
    "# Since there is a 90 000 token limit, we need to split our 200 samples into chunks\n",
    "# Function to split a list into smaller chunks\n",
    "def split_list(lst, chunk_size):\n",
    "    for i in range(0, len(lst), chunk_size):\n",
    "        yield lst[i:i + chunk_size]\n",
    "\n",
    "# Path to the existing JSONL file\n",
    "input_file = \"Test_Data/Input_JSONL/input.jsonl\"\n",
    "\n",
    "# Read the existing JSONL file\n",
    "with open(input_file, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Number of chunks (in this case, 4)\n",
    "num_chunks = 2\n",
    "chunk_size = len(lines) // num_chunks\n",
    "\n",
    "# Split the lines into chunks\n",
    "chunks = list(split_list(lines, chunk_size))\n",
    "\n",
    "# Write each chunk to a new JSONL file\n",
    "for idx, chunk in enumerate(chunks):\n",
    "    filename = f\"batchinput_part_{idx + 1}.jsonl\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        for line in chunk:\n",
    "            f.write(line)\n",
    "    print(f\"{filename} file created with {len(chunk)} lines.\")\n",
    "\n",
    "print(f\"Total lines: {sum(len(chunk) for chunk in chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch input file uploaded with ID: file-A7lIsvehdNTwNdqpnnesIWt4\n",
      "Batch job created.\n",
      "Batch(id='batch_mNvrFzYMUglQNiBy9OYZoMpY', completion_window='24h', created_at=1724947688, endpoint='/v1/chat/completions', input_file_id='file-A7lIsvehdNTwNdqpnnesIWt4', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1725034088, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'Transcript image processing batch job, part 1.'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "api_key_path = \"openai_api_key.txt\"\n",
    "# Set your OpenAI API key\n",
    "with open(api_key_path, \"r\") as f:\n",
    "    api_key = f.read()\n",
    "\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "# Upload JSONL file\n",
    "with open(jsonl_path, \"rb\") as f:\n",
    "    batch_input_file = client.files.create(file=f, purpose='batch')\n",
    "\n",
    "batch_input_file_id = batch_input_file.id\n",
    "print(f\"Batch input file uploaded with ID: {batch_input_file_id}\")\n",
    "\n",
    "# Create batch job\n",
    "batch = client.batches.create(\n",
    "    input_file_id=batch_input_file_id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\",\n",
    "    metadata={\n",
    "        \"description\": \"Transcript image processing batch job, part 1.\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Batch job created.\")\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batch(id='batch_uTtjmaVn2mfulzyoNKgkIhmJ', completion_window='24h', created_at=1724947388, endpoint='/v1/chat/completions', input_file_id='file-hu6HcZ5dOI76sFZ8hMnpOxC5', object='batch', status='in_progress', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1725033788, failed_at=None, finalizing_at=None, in_progress_at=1724947388, metadata={'description': 'Transcript image processing batch job, part 1.'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=10, total=16))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check status\n",
    "client.batches.retrieve(\"batch_uTtjmaVn2mfulzyoNKgkIhmJ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking status of batch job with ID: batch_mNvrFzYMUglQNiBy9OYZoMpY...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'openai' has no attribute 'Batch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m batch_id \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mid\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChecking status of batch job with ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m batch_status \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_batch_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Wait for batch completion\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m batch_status\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompleted\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfailed\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "Cell \u001b[0;32mIn[43], line 4\u001b[0m, in \u001b[0;36mcheck_batch_status\u001b[0;34m(batch_id)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_batch_status\u001b[39m(batch_id):\n\u001b[1;32m      3\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check the status of a batch job and return the batch object.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBatch\u001b[49m\u001b[38;5;241m.\u001b[39mretrieve(batch_id)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'openai' has no attribute 'Batch'"
     ]
    }
   ],
   "source": [
    "# Function to check batch status\n",
    "def check_batch_status(batch_id):\n",
    "    \"\"\"Check the status of a batch job and return the batch object.\"\"\"\n",
    "    batch = openai.Batch.retrieve(batch_id)\n",
    "    return batch\n",
    "\n",
    "# Function to download file from OpenAI ad save locally\n",
    "def download_file(file_id, save_path):\n",
    "    \"\"\"Download a file from OpenAI and save it to the specified directory.\"\"\"\n",
    "    file_content = openai.File.download(file_id)\n",
    "    \n",
    "    # Write the file content to the specified path\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        f.write(file_content)\n",
    "    print(f\"File downloaded and saved to: {save_path}\")\n",
    "\n",
    "# Check batch status until it's completed\n",
    "batch_id = batch.id\n",
    "print(f\"Checking status of batch job with ID: {batch_id}...\")\n",
    "\n",
    "batch_status = check_batch_status(batch_id)\n",
    "\n",
    "# Wait for batch completion\n",
    "while batch_status.status not in ['completed', 'failed']:\n",
    "    print(f\"Current batch status: {batch_status.status}\")\n",
    "    batch_status = check_batch_status(batch_id)\n",
    "\n",
    "# Once the batch is completed, download the output file\n",
    "if batch_status.status == 'completed' and batch_status.output_file_id:\n",
    "    output_file_id = batch_status.output_file_id\n",
    "    output_directory = \"Test_Data/Output_JSONL\"  # Specify your output directory here\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    output_file_path = os.path.join(output_directory, \"output_file.jsonl\")\n",
    "    \n",
    "    # Download the output file\n",
    "    download_file(output_file_id, output_file_path)\n",
    "else:\n",
    "    print(f\"Batch job failed or there is no output file available. Status: {batch_status.status}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
