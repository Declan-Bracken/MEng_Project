{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9c24b47",
   "metadata": {},
   "source": [
    "This Script is designed to be run on a gpu cluster with no previous knowledge of the dependencies or source code required to run or train a mini-cpm model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36f5203a-2f34-4b1f-8ffc-04341e8b23a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'MiniCPM-V' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/OpenBMB/MiniCPM-V.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfe59751-1252-4384-975a-859b355e8fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.23.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.23.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.23.0->transformers)\n",
      "  Downloading fsspec-2024.6.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Downloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.23.3-py3-none-any.whl (401 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.7/401.7 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (775 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m775.1/775.1 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.6.0-py3-none-any.whl (176 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.9/176.9 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, safetensors, regex, fsspec, huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "Successfully installed fsspec-2024.6.0 huggingface-hub-0.23.3 regex-2024.5.15 safetensors-0.4.3 tokenizers-0.19.1 tqdm-4.66.4 transformers-4.41.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting deepspeed\n",
      "  Downloading deepspeed-0.14.3.tar.gz (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting hjson (from deepspeed)\n",
      "  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting ninja (from deepspeed)\n",
      "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.24.1)\n",
      "Collecting nvidia-ml-py (from deepspeed)\n",
      "  Downloading nvidia_ml_py-12.555.43-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from deepspeed) (5.9.6)\n",
      "Collecting py-cpuinfo (from deepspeed)\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Collecting pydantic (from deepspeed)\n",
      "  Downloading pydantic-2.7.4-py3-none-any.whl.metadata (109 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.4/109.4 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from deepspeed) (2.1.0+cu118)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deepspeed) (4.66.4)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic->deepspeed)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.18.4 (from pydantic->deepspeed)\n",
      "  Downloading pydantic_core-2.18.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting typing-extensions>=4.6.1 (from pydantic->deepspeed)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (2024.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->deepspeed) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->deepspeed) (1.3.0)\n",
      "Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_ml_py-12.555.43-py3-none-any.whl (39 kB)\n",
      "Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Downloading pydantic-2.7.4-py3-none-any.whl (409 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.0/409.0 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.18.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Building wheels for collected packages: deepspeed\n",
      "  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.14.3-py3-none-any.whl size=1443096 sha256=1684d8e645d120ab82257d5bd92f1257ad2de252aa8d6f037eb14532f1bd1dcf\n",
      "  Stored in directory: /root/.cache/pip/wheels/82/bb/60/29f264d7ca7f5f2f3c9c1a6275883578977c0f608f1dee0790\n",
      "Successfully built deepspeed\n",
      "Installing collected packages: py-cpuinfo, nvidia-ml-py, ninja, hjson, typing-extensions, annotated-types, pydantic-core, pydantic, deepspeed\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "Successfully installed annotated-types-0.7.0 deepspeed-0.14.3 hjson-3.1.0 ninja-1.11.1.1 nvidia-ml-py-12.555.43 py-cpuinfo-9.0.0 pydantic-2.7.4 pydantic-core-2.18.4 typing-extensions-4.12.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting peft\n",
      "  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.1.0+cu118)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.41.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.4)\n",
      "Collecting accelerate>=0.21.0 (from peft)\n",
      "  Downloading accelerate-0.31.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.23.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2024.6.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.1.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.19.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "Downloading peft-0.11.1-py3-none-any.whl (251 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: accelerate, peft\n",
      "Successfully installed accelerate-0.31.0 peft-0.11.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.31.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2024.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.17.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting absl-py>=0.4 (from tensorboard)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard)\n",
      "  Downloading grpcio-1.64.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard)\n",
      "  Downloading Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.24.1)\n",
      "Collecting protobuf!=4.24.0,<5.0.0,>=3.19.6 (from tensorboard)\n",
      "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (68.2.2)\n",
      "Requirement already satisfied: six>1.9 in /usr/lib/python3/dist-packages (from tensorboard) (1.16.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard)\n",
      "  Downloading werkzeug-3.0.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.2)\n",
      "Downloading tensorboard-2.17.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.64.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.0.3-py3-none-any.whl (227 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.3/227.3 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: werkzeug, tensorboard-data-server, protobuf, markdown, grpcio, absl-py, tensorboard\n",
      "Successfully installed absl-py-2.1.0 grpcio-1.64.1 markdown-3.6 protobuf-4.25.3 tensorboard-2.17.0 tensorboard-data-server-0.7.2 werkzeug-3.0.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip install deepspeed\n",
    "!pip install peft\n",
    "!pip install accelerate\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8dc508f-bd73-490b-ac39-b4f484ef7834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'MEng_Project'...\n",
      "remote: Enumerating objects: 513, done.\u001b[K\n",
      "remote: Counting objects: 100% (513/513), done.\u001b[K\n",
      "remote: Compressing objects: 100% (369/369), done.\u001b[K\n",
      "remote: Total 513 (delta 198), reused 454 (delta 140), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (513/513), 33.51 MiB | 53.28 MiB/s, done.\n",
      "Resolving deltas: 100% (198/198), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://Declan-Bracken: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@github.com/Declan-Bracken/MEng_Project.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dccc0d5c-1200-488a-8656-24612e68e63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image paths updated successfully.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "# Split into training and eval\n",
    "\n",
    "orig_data_path = '/workspace/MEng_Project/Synthetic_Image_Annotation/Cleaned_JSON/cleaned_transcripts.json'\n",
    "\n",
    "# Load your JSON data\n",
    "with open(orig_data_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Define the new path prefix\n",
    "new_path_prefix = '/workspace/MEng_Project/'\n",
    "\n",
    "# Update the image paths\n",
    "for entry in data:\n",
    "    entry['image'] = new_path_prefix + entry['image']\n",
    "\n",
    "# Save the updated JSON data back to the file\n",
    "with open('modified_data.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4)\n",
    "\n",
    "print(\"Image paths updated successfully.\")\n",
    "\n",
    "# Load your dataset\n",
    "with open('modified_data.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "# Shuffle the data\n",
    "random.shuffle(data)\n",
    "\n",
    "# Determine the split point\n",
    "split_index = int(0.8 * len(data))\n",
    "\n",
    "# Split the data\n",
    "train_data = data[:split_index]\n",
    "val_data = data[split_index:]\n",
    "\n",
    "# Save the split data\n",
    "train_path = \"train_data.json\"\n",
    "val_path = \"val_data.json\"\n",
    "\n",
    "with open(train_path, 'w') as train_file:\n",
    "    json.dump(train_data, train_file, indent=4)\n",
    "\n",
    "with open(val_path, 'w') as val_file:\n",
    "    json.dump(val_data, val_file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ee61dac-16bc-4eba-98e4-59ac2d41e92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "script = \"\"\"\n",
    "#!/bin/bash\n",
    "\n",
    "GPUS_PER_NODE=2 # Kaggle typically provides 2 GPUs\n",
    "NNODES=1\n",
    "NODE_RANK=0\n",
    "MASTER_ADDR=localhost\n",
    "MASTER_PORT=6001 # Ensure this port is not blocked\n",
    "\n",
    "CHECKPOINT_DIR=\"workspace/output/output_minicpmv2/checkpoint-10\"\n",
    "\n",
    "MODEL=\"openbmb/MiniCPM-Llama3-V-2_5\" # or openbmb/MiniCPM-V-2\n",
    "DATA=\"/workspace/train_data.json\" # Update with actual path\n",
    "EVAL_DATA=\"/workspace/val_data.json\" # Update with actual path\n",
    "LLM_TYPE=\"llama3\" # if use openbmb/MiniCPM-V-2, please set LLM_TYPE=minicpm\n",
    "\n",
    "DISTRIBUTED_ARGS=\"\n",
    "    --nproc_per_node $GPUS_PER_NODE \\\n",
    "    --nnodes $NNODES \\\n",
    "    --node_rank $NODE_RANK \\\n",
    "    --master_addr $MASTER_ADDR \\\n",
    "    --master_port $MASTER_PORT\n",
    "\"\n",
    "\n",
    "torchrun $DISTRIBUTED_ARGS /workspace/Meng_Project/code_base/Synthetic_Image_Annotation/train.py  \\\n",
    "    --model_name_or_path $MODEL \\\n",
    "    --llm_type $LLM_TYPE \\\n",
    "    --data_path $DATA \\\n",
    "    --eval_data_path $EVAL_DATA \\\n",
    "    --remove_unused_columns false \\\n",
    "    --label_names \"labels\" \\\n",
    "    --prediction_loss_only false \\\n",
    "    --bf16 false \\\n",
    "    --bf16_full_eval false \\\n",
    "    --fp16 true \\\n",
    "    --fp16_full_eval true \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --tune_vision false \\\n",
    "    --tune_llm true \\\n",
    "    --model_max_length 1200 \\\n",
    "    --max_slice_nums 9 \\\n",
    "    --max_steps 500 \\\n",
    "    --eval_steps 10 \\\n",
    "    --output_dir /workspace/output/output_minicpmv2 \\\n",
    "    --logging_dir /workspace/output/output_minicpmv2 \\\n",
    "    --logging_strategy \"steps\" \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --gradient_accumulation_steps 1 \\\n",
    "    --evaluation_strategy \"steps\" \\\n",
    "    --save_strategy \"steps\" \\\n",
    "    --save_steps 10 \\\n",
    "    --save_total_limit 1 \\\n",
    "    --learning_rate 1e-6 \\\n",
    "    --weight_decay 0.1 \\\n",
    "    --adam_beta2 0.95 \\\n",
    "    --warmup_ratio 0.01 \\\n",
    "    --lr_scheduler_type \"cosine\" \\\n",
    "    --logging_steps 1 \\\n",
    "    --gradient_checkpointing true \\\n",
    "    --deepspeed /workspace/MiniCPM-V/finetune/ds_config_zero3.json \\\n",
    "    --report_to \"tensorboard\"\n",
    "\"\"\"\n",
    "\n",
    "with open(\"finetune_ds.sh\", \"w\") as file:\n",
    "    file.write(script)\n",
    "    \n",
    "!chmod +x finetune_ds.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0702322c-c5c4-4beb-b743-3332d6c9b7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-06-12 21:07:45,434] torch.distributed.run: [WARNING] \n",
      "[2024-06-12 21:07:45,434] torch.distributed.run: [WARNING] *****************************************\n",
      "[2024-06-12 21:07:45,434] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "[2024-06-12 21:07:45,434] torch.distributed.run: [WARNING] *****************************************\n",
      "[2024-06-12 21:07:48,444] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-06-12 21:07:48,451] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "df: /root/.triton/autotune: No such file or directory\n",
      "df: /root/.triton/autotune: No such file or directory\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "[2024-06-12 21:07:49,380] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-06-12 21:07:49,380] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "[2024-06-12 21:07:49,475] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "config.json: 100%|█████████████████████████| 1.37k/1.37k [00:00<00:00, 7.63MB/s]\n",
      "configuration_minicpm.py: 100%|████████████| 4.06k/4.06k [00:00<00:00, 31.9MB/s]\n",
      "A new version of the following files was downloaded from https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5:\n",
      "- configuration_minicpm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5:\n",
      "- configuration_minicpm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "modeling_minicpmv.py: 100%|████████████████| 25.0k/25.0k [00:00<00:00, 18.4MB/s]\n",
      "resampler.py: 100%|████████████████████████| 5.49k/5.49k [00:00<00:00, 28.0MB/s]\n",
      "A new version of the following files was downloaded from https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5:\n",
      "- resampler.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5:\n",
      "- modeling_minicpmv.py\n",
      "- resampler.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5:\n",
      "- resampler.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5:\n",
      "- modeling_minicpmv.py\n",
      "- resampler.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "model.safetensors.index.json: 100%|████████| 62.8k/62.8k [00:00<00:00, 15.0MB/s]\n",
      "Downloading shards:   0%|                                 | 0/7 [00:00<?, ?it/s]\n",
      "model-00001-of-00007.safetensors:   0%|             | 0.00/2.44G [00:00<?, ?B/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:   1%|     | 21.0M/2.44G [00:00<00:15, 155MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:   2%|     | 41.9M/2.44G [00:00<00:15, 156MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:   3%|▏    | 62.9M/2.44G [00:00<00:14, 168MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:   4%|▏    | 94.4M/2.44G [00:00<00:13, 173MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:   5%|▎     | 115M/2.44G [00:00<00:13, 174MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:   6%|▎     | 136M/2.44G [00:00<00:12, 182MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:   6%|▍     | 157M/2.44G [00:00<00:12, 182MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:   7%|▍     | 178M/2.44G [00:01<00:12, 182MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:   8%|▍     | 199M/2.44G [00:01<00:12, 177MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:   9%|▌     | 220M/2.44G [00:01<00:13, 161MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  10%|▌     | 241M/2.44G [00:01<00:13, 165MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  11%|▋     | 273M/2.44G [00:01<00:12, 175MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  12%|▋     | 294M/2.44G [00:01<00:12, 172MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  13%|▊     | 315M/2.44G [00:01<00:11, 180MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  14%|▊     | 336M/2.44G [00:01<00:11, 180MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  15%|▉     | 357M/2.44G [00:02<00:11, 177MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  16%|▉     | 388M/2.44G [00:02<00:11, 187MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  17%|█     | 409M/2.44G [00:02<00:10, 186MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  18%|█     | 430M/2.44G [00:02<00:11, 179MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  19%|█▏    | 461M/2.44G [00:02<00:09, 202MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  20%|█▏    | 493M/2.44G [00:02<00:08, 226MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  21%|█▎    | 524M/2.44G [00:02<00:09, 207MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  23%|█▎    | 556M/2.44G [00:03<00:09, 207MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  24%|█▍    | 587M/2.44G [00:03<00:09, 196MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  25%|█▍    | 608M/2.44G [00:03<00:09, 196MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  26%|█▌    | 629M/2.44G [00:03<00:09, 188MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  27%|█▌    | 650M/2.44G [00:03<00:10, 179MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  27%|█▋    | 671M/2.44G [00:03<00:09, 182MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  28%|█▋    | 692M/2.44G [00:03<00:09, 183MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  29%|█▊    | 713M/2.44G [00:03<00:09, 176MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  30%|█▊    | 734M/2.44G [00:04<00:09, 184MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  31%|█▊    | 755M/2.44G [00:04<00:09, 178MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  32%|█▉    | 776M/2.44G [00:04<00:09, 184MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  33%|█▉    | 797M/2.44G [00:04<00:08, 187MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  33%|██    | 818M/2.44G [00:04<00:09, 172MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  34%|██    | 839M/2.44G [00:04<00:09, 168MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  35%|██    | 860M/2.44G [00:04<00:09, 169MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  36%|██▏   | 881M/2.44G [00:04<00:09, 163MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  37%|██▏   | 902M/2.44G [00:04<00:09, 169MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  38%|██▎   | 923M/2.44G [00:05<00:08, 172MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  39%|██▎   | 944M/2.44G [00:05<00:08, 173MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  39%|██▎   | 965M/2.44G [00:05<00:08, 177MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  40%|██▍   | 986M/2.44G [00:05<00:07, 185MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  41%|██   | 1.01G/2.44G [00:05<00:07, 182MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  42%|██   | 1.03G/2.44G [00:05<00:07, 186MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  43%|██▏  | 1.06G/2.44G [00:05<00:07, 189MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  44%|██▏  | 1.08G/2.44G [00:05<00:07, 171MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  45%|██▎  | 1.10G/2.44G [00:06<00:07, 172MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  46%|██▎  | 1.12G/2.44G [00:06<00:07, 171MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  47%|██▎  | 1.14G/2.44G [00:06<00:07, 169MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  48%|██▍  | 1.16G/2.44G [00:06<00:07, 171MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  48%|██▍  | 1.18G/2.44G [00:06<00:07, 163MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  49%|██▍  | 1.21G/2.44G [00:06<00:07, 168MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  50%|██▌  | 1.23G/2.44G [00:06<00:06, 174MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  51%|██▌  | 1.25G/2.44G [00:06<00:07, 166MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  52%|██▌  | 1.27G/2.44G [00:07<00:06, 174MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  53%|██▋  | 1.30G/2.44G [00:07<00:06, 183MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  54%|██▋  | 1.32G/2.44G [00:07<00:06, 173MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  55%|██▋  | 1.34G/2.44G [00:07<00:06, 175MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  56%|██▊  | 1.36G/2.44G [00:07<00:05, 180MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  57%|██▊  | 1.38G/2.44G [00:07<00:05, 185MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  58%|██▉  | 1.41G/2.44G [00:07<00:05, 186MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  58%|██▉  | 1.43G/2.44G [00:07<00:05, 190MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  59%|██▉  | 1.45G/2.44G [00:08<00:05, 194MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  60%|███  | 1.47G/2.44G [00:08<00:05, 191MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  61%|███  | 1.49G/2.44G [00:08<00:04, 191MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  62%|███  | 1.51G/2.44G [00:08<00:05, 185MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  63%|███▏ | 1.53G/2.44G [00:08<00:05, 179MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  64%|███▏ | 1.55G/2.44G [00:08<00:05, 175MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  64%|███▏ | 1.57G/2.44G [00:08<00:04, 179MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  65%|███▎ | 1.59G/2.44G [00:08<00:04, 187MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  66%|███▎ | 1.61G/2.44G [00:08<00:04, 185MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  67%|███▎ | 1.64G/2.44G [00:09<00:04, 184MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  68%|███▍ | 1.66G/2.44G [00:09<00:04, 189MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  69%|███▍ | 1.68G/2.44G [00:09<00:04, 183MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  70%|███▍ | 1.70G/2.44G [00:09<00:04, 180MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  70%|███▌ | 1.72G/2.44G [00:09<00:03, 183MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  71%|███▌ | 1.74G/2.44G [00:09<00:03, 183MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  72%|███▌ | 1.76G/2.44G [00:09<00:03, 182MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  73%|███▋ | 1.78G/2.44G [00:09<00:03, 171MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  74%|███▋ | 1.80G/2.44G [00:10<00:03, 173MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  75%|███▋ | 1.82G/2.44G [00:10<00:03, 169MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  76%|███▊ | 1.85G/2.44G [00:10<00:03, 156MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  76%|███▊ | 1.87G/2.44G [00:10<00:03, 155MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  77%|███▊ | 1.89G/2.44G [00:10<00:03, 151MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  78%|███▉ | 1.91G/2.44G [00:10<00:03, 153MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  79%|███▉ | 1.93G/2.44G [00:10<00:03, 157MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  80%|███▉ | 1.95G/2.44G [00:10<00:03, 157MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  81%|████ | 1.97G/2.44G [00:11<00:02, 160MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  82%|████ | 1.99G/2.44G [00:11<00:02, 168MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  82%|████ | 2.01G/2.44G [00:11<00:02, 174MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  83%|████▏| 2.03G/2.44G [00:11<00:02, 178MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  84%|████▏| 2.06G/2.44G [00:11<00:02, 180MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  85%|████▏| 2.08G/2.44G [00:11<00:01, 186MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  86%|████▎| 2.10G/2.44G [00:11<00:01, 193MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  87%|████▎| 2.12G/2.44G [00:11<00:01, 189MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  88%|████▍| 2.14G/2.44G [00:11<00:01, 195MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  88%|████▍| 2.16G/2.44G [00:12<00:01, 199MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  90%|████▍| 2.19G/2.44G [00:12<00:01, 204MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  91%|████▌| 2.21G/2.44G [00:12<00:01, 194MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  91%|████▌| 2.23G/2.44G [00:12<00:01, 185MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  92%|████▌| 2.25G/2.44G [00:12<00:01, 186MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  94%|████▋| 2.29G/2.44G [00:12<00:00, 207MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  95%|████▊| 2.33G/2.44G [00:12<00:00, 245MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  97%|████▊| 2.36G/2.44G [00:12<00:00, 236MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors:  98%|████▉| 2.39G/2.44G [00:13<00:00, 226MB/s]\u001b[A\n",
      "model-00001-of-00007.safetensors: 100%|█████| 2.44G/2.44G [00:13<00:00, 182MB/s]\u001b[A\n",
      "Downloading shards:  14%|███▌                     | 1/7 [00:13<01:21, 13.55s/it]\n",
      "model-00002-of-00007.safetensors:   0%|             | 0.00/2.42G [00:00<?, ?B/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:   1%|     | 31.5M/2.42G [00:00<00:11, 205MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:   2%|     | 52.4M/2.42G [00:00<00:12, 188MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:   3%|▏    | 73.4M/2.42G [00:00<00:13, 179MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:   4%|▏    | 94.4M/2.42G [00:00<00:12, 179MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:   5%|▎     | 115M/2.42G [00:00<00:13, 172MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:   6%|▎     | 136M/2.42G [00:00<00:13, 170MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:   7%|▍     | 157M/2.42G [00:00<00:12, 179MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:   7%|▍     | 178M/2.42G [00:00<00:12, 177MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:   8%|▍     | 199M/2.42G [00:01<00:12, 177MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:   9%|▌     | 220M/2.42G [00:01<00:12, 181MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  10%|▌     | 252M/2.42G [00:01<00:11, 190MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  11%|▋     | 273M/2.42G [00:01<00:11, 188MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  12%|▋     | 294M/2.42G [00:01<00:11, 192MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  13%|▊     | 325M/2.42G [00:01<00:10, 201MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  14%|▊     | 346M/2.42G [00:01<00:10, 197MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  15%|▉     | 367M/2.42G [00:01<00:11, 185MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  16%|▉     | 388M/2.42G [00:02<00:10, 184MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  17%|█     | 409M/2.42G [00:02<00:10, 183MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  19%|█     | 451M/2.42G [00:02<00:08, 236MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  20%|█▏    | 493M/2.42G [00:02<00:07, 268MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  22%|█▎    | 535M/2.42G [00:02<00:06, 290MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  24%|█▍    | 577M/2.42G [00:02<00:05, 316MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  26%|█▌    | 619M/2.42G [00:02<00:05, 338MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  27%|█▋    | 661M/2.42G [00:02<00:05, 341MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  29%|█▋    | 703M/2.42G [00:03<00:04, 353MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  31%|█▊    | 744M/2.42G [00:03<00:04, 361MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  33%|█▉    | 786M/2.42G [00:03<00:04, 366MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  34%|██    | 828M/2.42G [00:03<00:04, 372MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  36%|██▏   | 881M/2.42G [00:03<00:03, 386MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  38%|██▎   | 923M/2.42G [00:03<00:03, 395MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  40%|██▍   | 965M/2.42G [00:03<00:03, 392MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  42%|██   | 1.01G/2.42G [00:03<00:03, 385MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  43%|██▏  | 1.05G/2.42G [00:03<00:03, 389MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  45%|██▎  | 1.09G/2.42G [00:04<00:03, 392MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  47%|██▎  | 1.13G/2.42G [00:04<00:03, 364MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  49%|██▍  | 1.18G/2.42G [00:04<00:03, 382MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  51%|██▌  | 1.23G/2.42G [00:04<00:03, 386MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  53%|██▋  | 1.27G/2.42G [00:04<00:03, 382MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  54%|██▋  | 1.31G/2.42G [00:04<00:02, 384MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  56%|██▊  | 1.35G/2.42G [00:04<00:02, 384MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  58%|██▉  | 1.39G/2.42G [00:04<00:02, 376MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  59%|██▉  | 1.44G/2.42G [00:04<00:02, 373MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  61%|███  | 1.48G/2.42G [00:05<00:02, 383MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  63%|███▏ | 1.52G/2.42G [00:05<00:02, 383MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  65%|███▏ | 1.56G/2.42G [00:05<00:02, 380MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  66%|███▎ | 1.60G/2.42G [00:05<00:02, 388MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  69%|███▍ | 1.66G/2.42G [00:05<00:01, 405MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  70%|███▌ | 1.70G/2.42G [00:05<00:01, 394MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  72%|███▌ | 1.74G/2.42G [00:05<00:01, 388MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  74%|███▋ | 1.78G/2.42G [00:05<00:01, 380MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  76%|███▊ | 1.82G/2.42G [00:05<00:01, 368MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  77%|███▊ | 1.87G/2.42G [00:06<00:01, 370MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  79%|███▉ | 1.91G/2.42G [00:06<00:01, 378MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  81%|████ | 1.95G/2.42G [00:06<00:01, 371MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  82%|████ | 1.99G/2.42G [00:06<00:01, 358MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  84%|████▏| 2.03G/2.42G [00:06<00:01, 361MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  86%|████▎| 2.08G/2.42G [00:06<00:00, 367MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  88%|████▍| 2.12G/2.42G [00:06<00:00, 370MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  89%|████▍| 2.16G/2.42G [00:06<00:00, 381MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  92%|████▌| 2.21G/2.42G [00:06<00:00, 397MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  93%|████▋| 2.25G/2.42G [00:07<00:00, 388MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  95%|████▊| 2.30G/2.42G [00:07<00:00, 373MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors:  97%|████▊| 2.34G/2.42G [00:07<00:00, 371MB/s]\u001b[A\n",
      "model-00002-of-00007.safetensors: 100%|█████| 2.42G/2.42G [00:07<00:00, 320MB/s]\u001b[A\n",
      "Downloading shards:  29%|███████▏                 | 2/7 [00:21<00:50, 10.07s/it]\n",
      "model-00003-of-00007.safetensors:   0%|             | 0.00/2.50G [00:00<?, ?B/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:   2%|     | 41.9M/2.50G [00:00<00:06, 360MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:   3%|▏    | 83.9M/2.50G [00:00<00:07, 331MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:   5%|▎     | 126M/2.50G [00:00<00:07, 334MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:   7%|▍     | 168M/2.50G [00:00<00:06, 356MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:   8%|▌     | 210M/2.50G [00:00<00:06, 364MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  10%|▌     | 252M/2.50G [00:00<00:06, 362MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  12%|▋     | 294M/2.50G [00:00<00:06, 364MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  13%|▊     | 336M/2.50G [00:00<00:06, 355MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  15%|▉     | 377M/2.50G [00:01<00:05, 357MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  17%|█     | 419M/2.50G [00:01<00:09, 231MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  18%|█     | 451M/2.50G [00:01<00:08, 238MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  19%|█▏    | 482M/2.50G [00:01<00:08, 244MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  21%|█▏    | 514M/2.50G [00:01<00:08, 246MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  22%|█▎    | 545M/2.50G [00:01<00:07, 255MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  23%|█▍    | 587M/2.50G [00:01<00:06, 287MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  26%|█▌    | 640M/2.50G [00:02<00:05, 332MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  27%|█▋    | 682M/2.50G [00:02<00:05, 339MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  29%|█▋    | 724M/2.50G [00:02<00:07, 233MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  30%|█▊    | 755M/2.50G [00:02<00:07, 246MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  32%|█▉    | 797M/2.50G [00:02<00:06, 277MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  34%|██    | 839M/2.50G [00:02<00:05, 300MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  35%|██    | 881M/2.50G [00:02<00:05, 311MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  37%|██▏   | 923M/2.50G [00:03<00:04, 328MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  39%|██▎   | 965M/2.50G [00:03<00:04, 340MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  40%|██   | 1.01G/2.50G [00:03<00:04, 333MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  42%|██   | 1.05G/2.50G [00:03<00:04, 346MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  44%|██▏  | 1.09G/2.50G [00:03<00:04, 348MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  45%|██▎  | 1.13G/2.50G [00:03<00:03, 355MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  47%|██▎  | 1.17G/2.50G [00:03<00:03, 361MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  49%|██▍  | 1.23G/2.50G [00:03<00:03, 375MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  51%|██▌  | 1.27G/2.50G [00:04<00:03, 373MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  52%|██▌  | 1.31G/2.50G [00:04<00:03, 371MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  54%|██▋  | 1.35G/2.50G [00:04<00:03, 348MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  56%|██▊  | 1.39G/2.50G [00:04<00:03, 345MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  57%|██▊  | 1.44G/2.50G [00:04<00:03, 336MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  59%|██▉  | 1.48G/2.50G [00:04<00:03, 328MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  61%|███  | 1.52G/2.50G [00:04<00:03, 316MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  62%|███  | 1.56G/2.50G [00:04<00:03, 304MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  64%|███▏ | 1.60G/2.50G [00:05<00:02, 306MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  65%|███▎ | 1.64G/2.50G [00:05<00:02, 306MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  67%|███▎ | 1.67G/2.50G [00:05<00:02, 305MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  68%|███▍ | 1.70G/2.50G [00:05<00:02, 296MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  69%|███▍ | 1.73G/2.50G [00:05<00:02, 301MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  71%|███▌ | 1.77G/2.50G [00:05<00:02, 317MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  73%|███▋ | 1.81G/2.50G [00:05<00:02, 291MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  74%|███▋ | 1.85G/2.50G [00:05<00:02, 293MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  76%|███▊ | 1.89G/2.50G [00:06<00:02, 303MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  77%|███▊ | 1.93G/2.50G [00:06<00:01, 310MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  79%|███▉ | 1.97G/2.50G [00:06<00:01, 313MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  80%|████ | 2.00G/2.50G [00:06<00:01, 312MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  82%|████ | 2.04G/2.50G [00:06<00:01, 323MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  83%|████▏| 2.09G/2.50G [00:06<00:01, 298MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  85%|████▏| 2.12G/2.50G [00:06<00:01, 297MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  86%|████▎| 2.16G/2.50G [00:06<00:01, 309MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  88%|████▍| 2.20G/2.50G [00:07<00:00, 322MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  90%|████▍| 2.24G/2.50G [00:07<00:00, 325MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  91%|████▌| 2.29G/2.50G [00:07<00:00, 336MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  93%|████▋| 2.33G/2.50G [00:07<00:00, 341MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  95%|████▋| 2.37G/2.50G [00:07<00:00, 346MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  96%|████▊| 2.41G/2.50G [00:07<00:00, 351MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors:  98%|████▉| 2.45G/2.50G [00:07<00:00, 350MB/s]\u001b[A\n",
      "model-00003-of-00007.safetensors: 100%|█████| 2.50G/2.50G [00:07<00:00, 317MB/s]\u001b[A\n",
      "Downloading shards:  43%|██████████▋              | 3/7 [00:29<00:36,  9.09s/it]\n",
      "model-00004-of-00007.safetensors:   0%|             | 0.00/2.50G [00:00<?, ?B/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:   2%|     | 41.9M/2.50G [00:00<00:06, 353MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:   3%|▏    | 83.9M/2.50G [00:00<00:06, 366MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:   5%|▎     | 126M/2.50G [00:00<00:06, 386MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:   7%|▍     | 168M/2.50G [00:00<00:06, 379MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:   8%|▌     | 210M/2.50G [00:00<00:06, 378MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  10%|▌     | 252M/2.50G [00:00<00:05, 379MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  12%|▋     | 294M/2.50G [00:00<00:05, 382MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  13%|▊     | 336M/2.50G [00:00<00:05, 374MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  15%|▉     | 377M/2.50G [00:00<00:05, 384MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  17%|█     | 419M/2.50G [00:01<00:05, 382MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  18%|█     | 461M/2.50G [00:01<00:05, 386MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  20%|█▏    | 503M/2.50G [00:01<00:05, 382MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  22%|█▎    | 545M/2.50G [00:01<00:05, 368MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  23%|█▍    | 587M/2.50G [00:01<00:05, 374MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  25%|█▌    | 629M/2.50G [00:01<00:04, 383MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  27%|█▌    | 671M/2.50G [00:01<00:04, 383MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  29%|█▋    | 713M/2.50G [00:01<00:04, 388MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  30%|█▊    | 755M/2.50G [00:01<00:04, 391MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  32%|█▉    | 797M/2.50G [00:02<00:04, 374MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  34%|██    | 839M/2.50G [00:02<00:04, 368MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  35%|██    | 881M/2.50G [00:02<00:04, 371MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  37%|██▏   | 923M/2.50G [00:02<00:04, 381MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  39%|██▎   | 965M/2.50G [00:02<00:04, 363MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  40%|██   | 1.01G/2.50G [00:02<00:04, 366MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  42%|██   | 1.05G/2.50G [00:02<00:03, 363MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  44%|██▏  | 1.09G/2.50G [00:02<00:04, 349MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  45%|██▎  | 1.13G/2.50G [00:03<00:03, 365MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  47%|██▎  | 1.17G/2.50G [00:03<00:03, 367MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  49%|██▍  | 1.22G/2.50G [00:03<00:03, 360MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  50%|██▌  | 1.26G/2.50G [00:03<00:03, 373MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  52%|██▌  | 1.30G/2.50G [00:03<00:03, 383MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  54%|██▋  | 1.34G/2.50G [00:03<00:03, 373MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  55%|██▊  | 1.38G/2.50G [00:03<00:02, 374MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  57%|██▊  | 1.43G/2.50G [00:03<00:02, 380MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  59%|██▉  | 1.47G/2.50G [00:03<00:02, 386MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  61%|███  | 1.52G/2.50G [00:04<00:02, 399MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  62%|███  | 1.56G/2.50G [00:04<00:02, 402MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  64%|███▏ | 1.60G/2.50G [00:04<00:02, 406MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  66%|███▎ | 1.66G/2.50G [00:04<00:02, 408MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  68%|███▍ | 1.70G/2.50G [00:04<00:01, 409MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  70%|███▌ | 1.75G/2.50G [00:04<00:01, 412MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  72%|███▌ | 1.79G/2.50G [00:04<00:01, 385MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  73%|███▋ | 1.84G/2.50G [00:04<00:01, 385MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  75%|███▊ | 1.88G/2.50G [00:04<00:01, 381MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  77%|███▊ | 1.92G/2.50G [00:05<00:01, 384MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  78%|███▉ | 1.96G/2.50G [00:05<00:01, 376MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  80%|████ | 2.00G/2.50G [00:05<00:01, 338MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  82%|████ | 2.04G/2.50G [00:05<00:01, 331MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  83%|████▏| 2.09G/2.50G [00:05<00:01, 337MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  85%|████▎| 2.13G/2.50G [00:05<00:01, 348MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  87%|████▎| 2.17G/2.50G [00:05<00:00, 361MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  89%|████▍| 2.21G/2.50G [00:05<00:00, 367MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  90%|████▌| 2.25G/2.50G [00:06<00:00, 367MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  92%|████▌| 2.31G/2.50G [00:06<00:00, 387MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  94%|████▋| 2.35G/2.50G [00:06<00:00, 365MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  96%|████▊| 2.39G/2.50G [00:06<00:00, 320MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors:  97%|████▊| 2.43G/2.50G [00:06<00:00, 319MB/s]\u001b[A\n",
      "model-00004-of-00007.safetensors: 100%|█████| 2.50G/2.50G [00:06<00:00, 371MB/s]\u001b[A\n",
      "Downloading shards:  57%|██████████████▎          | 4/7 [00:35<00:24,  8.19s/it]\n",
      "Downloading shards:  57%|██████████████▎          | 4/7 [00:35<00:24,  8.20s/it]\u001b[A\n",
      "model-00005-of-00007.safetensors:   2%|     | 41.9M/2.42G [00:00<00:05, 414MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:   3%|▏    | 83.9M/2.42G [00:00<00:05, 397MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:   5%|▎     | 126M/2.42G [00:00<00:05, 394MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:   7%|▍     | 168M/2.42G [00:00<00:05, 397MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:   9%|▌     | 210M/2.42G [00:00<00:05, 377MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:  10%|▌     | 252M/2.42G [00:00<00:05, 377MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:  12%|▋     | 294M/2.42G [00:00<00:05, 376MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:  14%|▊     | 336M/2.42G [00:00<00:05, 368MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:  16%|▉     | 377M/2.42G [00:00<00:05, 382MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:  18%|█     | 430M/2.42G [00:01<00:05, 394MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:  20%|█▏    | 472M/2.42G [00:01<00:07, 254MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:  21%|█▎    | 514M/2.42G [00:01<00:06, 286MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:  23%|█▍    | 566M/2.42G [00:01<00:05, 320MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:  26%|█▌    | 619M/2.42G [00:01<00:05, 352MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:  28%|█▋    | 671M/2.42G [00:01<00:04, 371MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:  30%|█▊    | 713M/2.42G [00:01<00:04, 381MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:  31%|█▊    | 755M/2.42G [00:02<00:04, 374MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:  33%|█▉    | 797M/2.42G [00:02<00:04, 371MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:  35%|██    | 839M/2.42G [00:02<00:04, 357MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:  36%|██▏   | 881M/2.42G [00:02<00:04, 370MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:  38%|██▎   | 923M/2.42G [00:02<00:03, 379MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:  40%|██▍   | 965M/2.42G [00:02<00:03, 388MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:  42%|██   | 1.01G/2.42G [00:02<00:04, 331MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:  43%|██▏  | 1.05G/2.42G [00:02<00:03, 350MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:  45%|██▎  | 1.09G/2.42G [00:03<00:03, 368MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:  47%|██▎  | 1.13G/2.42G [00:03<00:03, 380MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:  49%|██▍  | 1.17G/2.42G [00:03<00:03, 375MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:  50%|██▌  | 1.22G/2.42G [00:03<00:03, 379MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:  52%|██▌  | 1.26G/2.42G [00:03<00:03, 374MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:  54%|██▋  | 1.30G/2.42G [00:03<00:03, 368MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:  56%|██▊  | 1.34G/2.42G [00:03<00:02, 367MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:  57%|██▊  | 1.38G/2.42G [00:03<00:02, 364MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:  59%|██▉  | 1.43G/2.42G [00:03<00:02, 360MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:  61%|███  | 1.47G/2.42G [00:04<00:02, 360MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:  62%|███  | 1.51G/2.42G [00:04<00:02, 364MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:  64%|███▏ | 1.55G/2.42G [00:04<00:02, 357MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:  66%|███▎ | 1.59G/2.42G [00:04<00:02, 359MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:  68%|███▍ | 1.64G/2.42G [00:04<00:02, 356MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:  69%|███▍ | 1.68G/2.42G [00:04<00:02, 352MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:  71%|███▌ | 1.72G/2.42G [00:04<00:01, 360MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:  73%|███▋ | 1.76G/2.42G [00:04<00:01, 352MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:  75%|███▋ | 1.80G/2.42G [00:05<00:01, 343MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:  76%|███▊ | 1.85G/2.42G [00:05<00:01, 338MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:  78%|███▉ | 1.89G/2.42G [00:05<00:01, 340MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:  80%|███▉ | 1.93G/2.42G [00:05<00:01, 337MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:  82%|████ | 1.97G/2.42G [00:05<00:01, 337MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:  83%|████▏| 2.01G/2.42G [00:05<00:01, 329MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:  85%|████▎| 2.06G/2.42G [00:05<00:01, 333MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:  87%|████▎| 2.10G/2.42G [00:05<00:00, 341MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:  89%|████▍| 2.14G/2.42G [00:06<00:00, 345MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:  90%|████▌| 2.18G/2.42G [00:06<00:00, 347MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:  92%|████▌| 2.22G/2.42G [00:06<00:00, 345MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:  94%|████▋| 2.26G/2.42G [00:06<00:00, 341MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:  95%|████▊| 2.31G/2.42G [00:06<00:00, 325MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors:  97%|████▊| 2.35G/2.42G [00:06<00:00, 330MB/s]\u001b[A\n",
      "model-00005-of-00007.safetensors: 100%|█████| 2.42G/2.42G [00:06<00:00, 353MB/s]\u001b[A\n",
      "Downloading shards:  71%|█████████████████▊       | 5/7 [00:42<00:15,  7.72s/it]\n",
      "model-00006-of-00007.safetensors:   0%|             | 0.00/2.50G [00:00<?, ?B/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:   2%|     | 52.4M/2.50G [00:00<00:05, 414MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:   4%|▏    | 94.4M/2.50G [00:00<00:06, 391MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:   5%|▎     | 136M/2.50G [00:00<00:05, 398MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:   8%|▍     | 189M/2.50G [00:00<00:05, 406MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:   9%|▌     | 231M/2.50G [00:00<00:05, 384MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  11%|▋     | 273M/2.50G [00:00<00:05, 374MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  13%|▊     | 315M/2.50G [00:00<00:05, 385MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  14%|▊     | 357M/2.50G [00:00<00:06, 355MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  16%|▉     | 398M/2.50G [00:01<00:06, 346MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  18%|█     | 440M/2.50G [00:01<00:05, 350MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  19%|█▏    | 482M/2.50G [00:01<00:05, 346MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  21%|█▎    | 524M/2.50G [00:01<00:05, 355MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  23%|█▎    | 566M/2.50G [00:01<00:05, 361MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  24%|█▍    | 608M/2.50G [00:01<00:05, 361MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  26%|█▌    | 650M/2.50G [00:01<00:05, 355MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  28%|█▋    | 692M/2.50G [00:01<00:05, 357MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  29%|█▊    | 734M/2.50G [00:02<00:05, 347MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  31%|█▊    | 776M/2.50G [00:02<00:05, 343MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  33%|█▉    | 818M/2.50G [00:02<00:04, 344MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  34%|██    | 860M/2.50G [00:02<00:04, 341MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  36%|██▏   | 902M/2.50G [00:02<00:04, 350MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  38%|██▎   | 944M/2.50G [00:02<00:04, 345MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  39%|██▎   | 986M/2.50G [00:02<00:04, 360MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  41%|██   | 1.03G/2.50G [00:02<00:03, 374MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  43%|██▏  | 1.07G/2.50G [00:02<00:03, 376MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  44%|██▏  | 1.11G/2.50G [00:03<00:03, 375MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  46%|██▎  | 1.15G/2.50G [00:03<00:03, 365MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  48%|██▍  | 1.20G/2.50G [00:03<00:03, 376MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  49%|██▍  | 1.24G/2.50G [00:03<00:03, 361MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  51%|██▌  | 1.28G/2.50G [00:03<00:03, 328MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  53%|██▋  | 1.32G/2.50G [00:03<00:03, 337MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  55%|██▋  | 1.36G/2.50G [00:03<00:03, 356MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  56%|██▊  | 1.41G/2.50G [00:03<00:02, 367MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  58%|██▉  | 1.45G/2.50G [00:04<00:02, 355MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  60%|██▉  | 1.49G/2.50G [00:04<00:02, 357MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  61%|███  | 1.53G/2.50G [00:04<00:02, 353MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  63%|███▏ | 1.57G/2.50G [00:04<00:02, 334MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  65%|███▏ | 1.61G/2.50G [00:04<00:02, 353MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  66%|███▎ | 1.66G/2.50G [00:04<00:02, 368MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  68%|███▍ | 1.70G/2.50G [00:04<00:02, 377MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  70%|███▍ | 1.74G/2.50G [00:04<00:01, 380MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  71%|███▌ | 1.78G/2.50G [00:04<00:01, 390MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  73%|███▋ | 1.82G/2.50G [00:05<00:01, 394MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  75%|███▊ | 1.88G/2.50G [00:05<00:01, 400MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  77%|███▊ | 1.92G/2.50G [00:05<00:01, 405MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  79%|███▉ | 1.97G/2.50G [00:05<00:01, 414MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  81%|████ | 2.01G/2.50G [00:05<00:01, 409MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  82%|████ | 2.06G/2.50G [00:05<00:01, 388MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  84%|████▏| 2.10G/2.50G [00:05<00:01, 361MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  86%|████▎| 2.14G/2.50G [00:05<00:00, 370MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  87%|████▎| 2.18G/2.50G [00:05<00:00, 367MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  89%|████▍| 2.22G/2.50G [00:06<00:00, 373MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  91%|████▌| 2.28G/2.50G [00:06<00:00, 381MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  93%|████▋| 2.32G/2.50G [00:06<00:00, 389MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  94%|████▋| 2.36G/2.50G [00:06<00:00, 391MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  96%|████▊| 2.40G/2.50G [00:06<00:00, 396MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors:  98%|████▉| 2.44G/2.50G [00:06<00:00, 392MB/s]\u001b[A\n",
      "model-00006-of-00007.safetensors: 100%|█████| 2.50G/2.50G [00:06<00:00, 370MB/s]\u001b[A\n",
      "Downloading shards:  86%|█████████████████████▍   | 6/7 [00:49<00:07,  7.41s/it]\n",
      "model-00007-of-00007.safetensors:   0%|             | 0.00/2.30G [00:00<?, ?B/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:   2%|     | 52.4M/2.30G [00:00<00:05, 395MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:   4%|▏    | 94.4M/2.30G [00:00<00:05, 375MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:   6%|▎     | 136M/2.30G [00:00<00:05, 378MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:   8%|▍     | 178M/2.30G [00:00<00:05, 377MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  10%|▌     | 220M/2.30G [00:00<00:05, 381MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  11%|▋     | 262M/2.30G [00:00<00:05, 381MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  13%|▊     | 304M/2.30G [00:00<00:05, 381MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  15%|▉     | 346M/2.30G [00:00<00:05, 366MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  17%|█     | 388M/2.30G [00:01<00:05, 377MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  19%|█     | 430M/2.30G [00:01<00:04, 384MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  21%|█▏    | 472M/2.30G [00:01<00:05, 349MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  22%|█▎    | 514M/2.30G [00:01<00:05, 324MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  24%|█▍    | 556M/2.30G [00:01<00:06, 252MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  26%|█▌    | 587M/2.30G [00:01<00:07, 239MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  27%|█▌    | 619M/2.30G [00:02<00:08, 198MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  28%|█▋    | 650M/2.30G [00:02<00:08, 200MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  30%|█▊    | 682M/2.30G [00:02<00:08, 180MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  31%|█▊    | 703M/2.30G [00:02<00:10, 157MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  31%|█▌   | 724M/2.30G [00:03<00:17, 88.1MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  32%|█▌   | 744M/2.30G [00:03<00:16, 95.9MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  33%|█▋   | 765M/2.30G [00:03<00:15, 99.8MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  34%|█▋   | 786M/2.30G [00:04<00:19, 75.7MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  35%|█▋   | 797M/2.30G [00:04<00:21, 70.4MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  35%|█▊   | 807M/2.30G [00:04<00:21, 70.6MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  36%|█▊   | 818M/2.30G [00:04<00:22, 64.9MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  36%|█▊   | 828M/2.30G [00:04<00:21, 67.7MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  36%|█▊   | 839M/2.30G [00:05<00:25, 56.3MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  37%|█▊   | 849M/2.30G [00:05<00:22, 63.2MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  38%|█▉   | 870M/2.30G [00:05<00:18, 76.7MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  38%|█▉   | 881M/2.30G [00:05<00:19, 73.2MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  39%|█▉   | 891M/2.30G [00:05<00:19, 72.8MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  39%|█▉   | 902M/2.30G [00:05<00:18, 74.7MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  40%|██   | 923M/2.30G [00:05<00:17, 80.1MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  41%|██   | 933M/2.30G [00:06<00:18, 75.1MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  41%|██   | 944M/2.30G [00:06<00:17, 79.2MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  41%|██   | 954M/2.30G [00:06<00:18, 71.5MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  42%|██   | 965M/2.30G [00:06<00:26, 49.9MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  42%|██   | 975M/2.30G [00:10<02:27, 8.98MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  43%|██▏  | 996M/2.30G [00:10<01:23, 15.6MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  44%|█▊  | 1.02G/2.30G [00:10<00:52, 24.3MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  46%|█▊  | 1.05G/2.30G [00:10<00:30, 40.5MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  47%|█▊  | 1.07G/2.30G [00:10<00:23, 53.2MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  48%|█▉  | 1.10G/2.30G [00:11<00:15, 76.5MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  49%|██▍  | 1.13G/2.30G [00:11<00:11, 103MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  51%|██▌  | 1.16G/2.30G [00:11<00:08, 127MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  52%|██▌  | 1.20G/2.30G [00:11<00:07, 148MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  53%|██▋  | 1.23G/2.30G [00:11<00:06, 165MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  55%|██▋  | 1.26G/2.30G [00:11<00:05, 176MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  56%|██▊  | 1.29G/2.30G [00:11<00:05, 172MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  57%|██▊  | 1.31G/2.30G [00:12<00:05, 170MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  58%|██▉  | 1.33G/2.30G [00:12<00:05, 170MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  59%|██▉  | 1.36G/2.30G [00:12<00:05, 182MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  60%|███  | 1.38G/2.30G [00:12<00:05, 172MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  61%|███  | 1.41G/2.30G [00:12<00:05, 150MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  62%|███  | 1.43G/2.30G [00:12<00:05, 156MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  63%|███▏ | 1.46G/2.30G [00:12<00:04, 175MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  65%|███▏ | 1.49G/2.30G [00:13<00:04, 199MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  66%|███▎ | 1.51G/2.30G [00:13<00:03, 200MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  67%|███▎ | 1.54G/2.30G [00:13<00:03, 213MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  68%|███▍ | 1.57G/2.30G [00:13<00:03, 224MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  70%|███▍ | 1.60G/2.30G [00:13<00:03, 207MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  71%|███▌ | 1.64G/2.30G [00:13<00:03, 206MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  73%|███▋ | 1.67G/2.30G [00:13<00:03, 209MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  74%|███▋ | 1.70G/2.30G [00:14<00:02, 215MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  75%|███▊ | 1.73G/2.30G [00:14<00:02, 221MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  77%|███▊ | 1.76G/2.30G [00:14<00:02, 216MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  78%|███▉ | 1.79G/2.30G [00:14<00:02, 220MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  79%|███▉ | 1.82G/2.30G [00:14<00:02, 212MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  81%|████ | 1.86G/2.30G [00:14<00:02, 206MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  82%|████ | 1.88G/2.30G [00:14<00:02, 194MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  83%|████▏| 1.90G/2.30G [00:15<00:02, 190MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  83%|████▏| 1.92G/2.30G [00:15<00:02, 185MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  84%|████▏| 1.94G/2.30G [00:15<00:02, 175MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  85%|████▎| 1.96G/2.30G [00:15<00:01, 175MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  86%|████▎| 1.98G/2.30G [00:15<00:01, 173MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  87%|████▎| 2.00G/2.30G [00:15<00:01, 174MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  88%|████▍| 2.02G/2.30G [00:15<00:01, 172MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  89%|████▍| 2.04G/2.30G [00:15<00:01, 178MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  90%|████▍| 2.07G/2.30G [00:15<00:01, 173MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  91%|████▌| 2.09G/2.30G [00:16<00:01, 143MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  92%|████▌| 2.11G/2.30G [00:16<00:01, 147MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  93%|████▋| 2.14G/2.30G [00:16<00:00, 167MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  94%|████▋| 2.16G/2.30G [00:16<00:00, 162MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  95%|████▋| 2.18G/2.30G [00:16<00:00, 169MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  96%|████▊| 2.21G/2.30G [00:16<00:00, 191MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  98%|████▉| 2.24G/2.30G [00:17<00:00, 202MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors:  99%|████▉| 2.26G/2.30G [00:17<00:00, 203MB/s]\u001b[A\n",
      "model-00007-of-00007.safetensors: 100%|█████| 2.30G/2.30G [00:17<00:00, 133MB/s]\u001b[A\n",
      "Downloading shards: 100%|█████████████████████████| 7/7 [01:06<00:00,  9.56s/it]\n",
      "Downloading shards: 100%|█████████████████████████| 7/7 [01:06<00:00,  9.56s/it]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/init.py:46: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1442.)\n",
      "  tensor.erfinv_()\n",
      "[2024-06-12 21:09:08,246] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 741, num_elems = 8.54B\n",
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [00:19<00:00,  2.76s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [00:19<00:00,  2.77s/it]\n",
      "generation_config.json: 100%|██████████████████| 121/121 [00:00<00:00, 1.20MB/s]\n",
      "tokenizer_config.json: 100%|███████████████| 50.9k/50.9k [00:00<00:00, 1.13MB/s]\n",
      "tokenizer.json: 100%|██████████████████████| 9.08M/9.08M [00:00<00:00, 12.0MB/s]\n",
      "special_tokens_map.json: 100%|█████████████████| 459/459 [00:00<00:00, 2.81MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "{'Total': 8537092336, 'Trainable': 8119300096}\n",
      "llm_type=llama3\n",
      "Loading data...\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "[2024-06-12 21:09:31,651] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs\n",
      "[2024-06-12 21:09:31,660] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs\n",
      "Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n",
      "Creating extension directory /root/.cache/torch_extensions/py310_cu118/cpu_adam...\n",
      "Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py310_cu118/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/3] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/usr/local/lib/python3.10/dist-packages/deepspeed/ops/csrc/includes -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX256__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /usr/local/lib/python3.10/dist-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o \n",
      "[2/3] c++ -MMD -MF cpu_adam_impl.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/usr/local/lib/python3.10/dist-packages/deepspeed/ops/csrc/includes -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX256__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /usr/local/lib/python3.10/dist-packages/deepspeed/ops/csrc/adam/cpu_adam_impl.cpp -o cpu_adam_impl.o \n",
      "[3/3] c++ cpu_adam.o cpu_adam_impl.o -shared -lcurand -L/usr/local/lib/python3.10/dist-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o cpu_adam.so\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 33.3472945690155 seconds\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 33.440335273742676 seconds\n",
      "Parameter Offload: Total persistent parameters: 706800 in 346 params\n",
      "  0%|                                                   | 0/500 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "  0%|                                         | 1/500 [00:57<7:58:28, 57.53s/it]tried to get lr value before scheduler/optimizer started stepping, returning lr=0\n",
      "tried to get lr value before scheduler/optimizer started stepping, returning lr=0\n",
      "{'loss': 0.6115, 'grad_norm': 0.0, 'learning_rate': 0, 'epoch': 0.02}           \n",
      "  0%|▏                                        | 2/500 [01:47<7:22:56, 53.37s/it]tried to get lr value before scheduler/optimizer started stepping, returning lr=0\n",
      "tried to get lr value before scheduler/optimizer started stepping, returning lr=0\n",
      "{'loss': 0.3864, 'grad_norm': 0.0, 'learning_rate': 0, 'epoch': 0.03}           \n",
      "  1%|▏                                        | 3/500 [02:36<7:03:42, 51.15s/it]tried to get lr value before scheduler/optimizer started stepping, returning lr=0\n",
      "tried to get lr value before scheduler/optimizer started stepping, returning lr=0\n",
      "{'loss': 0.6927, 'grad_norm': 0.0, 'learning_rate': 0, 'epoch': 0.05}           \n",
      "{'loss': 0.5341, 'grad_norm': 23.635250390786137, 'learning_rate': 4.30676558073393e-07, 'epoch': 0.08}\n",
      "{'loss': 0.3237, 'grad_norm': 18.32023170957666, 'learning_rate': 6.826061944859853e-07, 'epoch': 0.1}\n",
      "{'loss': 0.5046, 'grad_norm': 22.992808668979116, 'learning_rate': 8.61353116146786e-07, 'epoch': 0.12}\n",
      "{'loss': 0.6481, 'grad_norm': 22.992808668979116, 'learning_rate': 8.61353116146786e-07, 'epoch': 0.13}\n",
      "{'loss': 0.8268, 'grad_norm': 78.06457663370756, 'learning_rate': 1e-06, 'epoch': 0.15}\n",
      "{'loss': 0.6063, 'grad_norm': 21.040225059552967, 'learning_rate': 1e-06, 'epoch': 0.17}\n",
      "  2%|▊                                       | 10/500 [09:54<8:13:11, 60.39s/it]\n",
      "  0%|                                                    | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      " 13%|█████▊                                      | 2/15 [00:05<00:33,  2.56s/it]\u001b[A\n",
      " 20%|████████▊                                   | 3/15 [00:10<00:44,  3.73s/it]\u001b[A\n",
      " 27%|███████████▋                                | 4/15 [00:15<00:48,  4.37s/it]\u001b[A\n",
      " 33%|██████████████▋                             | 5/15 [00:21<00:47,  4.76s/it]\u001b[A\n",
      " 40%|█████████████████▌                          | 6/15 [00:26<00:45,  5.01s/it]\u001b[A\n",
      " 47%|████████████████████▌                       | 7/15 [00:32<00:41,  5.15s/it]\u001b[A\n",
      " 53%|███████████████████████▍                    | 8/15 [00:37<00:36,  5.25s/it]\u001b[A\n",
      " 60%|██████████████████████████▍                 | 9/15 [00:43<00:31,  5.33s/it]\u001b[A\n",
      " 67%|████████████████████████████▋              | 10/15 [00:48<00:26,  5.38s/it]\u001b[A\n",
      " 73%|███████████████████████████████▌           | 11/15 [00:54<00:21,  5.44s/it]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 12/15 [00:59<00:16,  5.45s/it]\u001b[A\n",
      " 87%|█████████████████████████████████████▎     | 13/15 [01:05<00:10,  5.47s/it]\u001b[A\n",
      " 93%|████████████████████████████████████████▏  | 14/15 [01:10<00:05,  5.49s/it]\u001b[A\n",
      "100%|███████████████████████████████████████████| 15/15 [01:16<00:00,  5.41s/it]\u001b[A\n",
      "{'eval_loss': 0.4745715260505676, 'eval_runtime': 82.5774, 'eval_samples_per_second': 0.363, 'eval_steps_per_second': 0.182, 'epoch': 0.17}\n",
      "\n",
      "  2%|▊                                       | 10/500 [11:17<8:13:11, 60.39s/it]\u001b[A\n",
      "                                                                                \u001b[A/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "{'loss': 0.362, 'grad_norm': 18.275514090804755, 'learning_rate': 1e-06, 'epoch': 0.18}\n",
      "{'loss': 0.7011, 'grad_norm': 21.939783395899433, 'learning_rate': 1e-06, 'epoch': 0.2}\n",
      "{'loss': 0.3093, 'grad_norm': 13.538998823241776, 'learning_rate': 1e-06, 'epoch': 0.22}\n",
      "{'loss': 0.3859, 'grad_norm': 15.066863508260852, 'learning_rate': 1e-06, 'epoch': 0.23}\n",
      "{'loss': 0.2982, 'grad_norm': 17.511916980391526, 'learning_rate': 1e-06, 'epoch': 0.25}\n",
      "{'loss': 0.2826, 'grad_norm': 17.70508835924277, 'learning_rate': 1e-06, 'epoch': 0.28}\n",
      "{'loss': 0.9326, 'grad_norm': 24.09449475989017, 'learning_rate': 1e-06, 'epoch': 0.3}\n",
      "{'loss': 0.7623, 'grad_norm': 16.93121063464416, 'learning_rate': 1e-06, 'epoch': 0.32}\n",
      "{'loss': 0.4869, 'grad_norm': 16.47139132489221, 'learning_rate': 1e-06, 'epoch': 0.33}\n",
      "  4%|█▌                                      | 20/500 [23:56<8:26:15, 63.28s/it]\n",
      "  0%|                                                    | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      " 13%|█████▊                                      | 2/15 [00:05<00:35,  2.76s/it]\u001b[A\n",
      " 20%|████████▊                                   | 3/15 [00:11<00:46,  3.90s/it]\u001b[A\n",
      " 27%|███████████▋                                | 4/15 [00:16<00:49,  4.47s/it]\u001b[A\n",
      " 33%|██████████████▋                             | 5/15 [00:22<00:49,  4.91s/it]\u001b[A\n",
      " 40%|█████████████████▌                          | 6/15 [00:27<00:46,  5.12s/it]\u001b[A/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "{'loss': 0.3214, 'grad_norm': 13.353254948686084, 'learning_rate': 1e-06, 'epoch': 0.35}\n",
      "{'loss': 0.3531, 'grad_norm': 15.60815644877678, 'learning_rate': 1e-06, 'epoch': 0.37}\n",
      "{'loss': 0.2338, 'grad_norm': 10.67603065821911, 'learning_rate': 1e-06, 'epoch': 0.38}\n",
      "{'loss': 0.2733, 'grad_norm': 14.185334834442026, 'learning_rate': 1e-06, 'epoch': 0.4}\n",
      "{'loss': 0.2923, 'grad_norm': 41.2581742019271, 'learning_rate': 1e-06, 'epoch': 0.42}\n",
      "{'loss': 0.5414, 'grad_norm': 31.043373528646374, 'learning_rate': 1e-06, 'epoch': 0.43}\n",
      "{'loss': 0.5977, 'grad_norm': 23.723534545016552, 'learning_rate': 1e-06, 'epoch': 0.45}\n",
      "{'loss': 0.2245, 'grad_norm': 10.5258615897717, 'learning_rate': 1e-06, 'epoch': 0.47}\n",
      "{'loss': 0.2496, 'grad_norm': 14.978248083451351, 'learning_rate': 1e-06, 'epoch': 0.48}\n",
      "{'loss': 0.2581, 'grad_norm': 13.904234733715963, 'learning_rate': 1e-06, 'epoch': 0.5}\n",
      "  6%|██▍                                     | 30/500 [38:01<8:24:56, 64.46s/it]\n",
      "  0%|                                                    | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      " 13%|█████▊                                      | 2/15 [00:05<00:35,  2.72s/it]\u001b[A\n",
      " 20%|████████▊                                   | 3/15 [00:10<00:46,  3.84s/it]\u001b[A\n",
      " 27%|███████████▋                                | 4/15 [00:16<00:48,  4.43s/it]\u001b[A\n",
      " 33%|██████████████▋                             | 5/15 [00:21<00:48,  4.84s/it]\u001b[A\n",
      " 40%|█████████████████▌                          | 6/15 [00:27<00:45,  5.04s/it]\u001b[A\n",
      " 47%|████████████████████▌                       | 7/15 [00:32<00:41,  5.25s/it]\u001b[A\n",
      " 53%|███████████████████████▍                    | 8/15 [00:38<00:37,  5.31s/it]\u001b[A\n",
      " 60%|██████████████████████████▍                 | 9/15 [00:44<00:32,  5.48s/it]\u001b[A\n",
      " 67%|████████████████████████████▋              | 10/15 [00:49<00:27,  5.45s/it]\u001b[A\n",
      " 73%|███████████████████████████████▌           | 11/15 [00:55<00:21,  5.44s/it]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 12/15 [01:01<00:17,  5.68s/it]\u001b[A\n",
      " 87%|█████████████████████████████████████▎     | 13/15 [01:06<00:11,  5.62s/it]\u001b[A\n",
      " 93%|████████████████████████████████████████▏  | 14/15 [01:12<00:05,  5.60s/it]\u001b[A\n",
      "100%|███████████████████████████████████████████| 15/15 [01:17<00:00,  5.46s/it]\u001b[A\n",
      "{'eval_loss': 0.36848002672195435, 'eval_runtime': 83.1811, 'eval_samples_per_second': 0.361, 'eval_steps_per_second': 0.18, 'epoch': 0.5}\n",
      "\n",
      "  6%|██▍                                     | 30/500 [39:24<8:24:56, 64.46s/it]\u001b[A\n",
      "                                                                                \u001b[A/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "{'loss': 0.1419, 'grad_norm': 7.805045027080617, 'learning_rate': 1e-06, 'epoch': 0.52}\n",
      "{'loss': 0.3105, 'grad_norm': 16.171216396613268, 'learning_rate': 1e-06, 'epoch': 0.53}\n",
      "{'loss': 0.4161, 'grad_norm': 19.385896633409814, 'learning_rate': 1e-06, 'epoch': 0.55}\n",
      "{'loss': 0.3111, 'grad_norm': 16.85224250337557, 'learning_rate': 1e-06, 'epoch': 0.57}\n",
      "{'loss': 0.5486, 'grad_norm': 20.96740518228531, 'learning_rate': 1e-06, 'epoch': 0.58}\n",
      "{'loss': 0.1977, 'grad_norm': 8.543503676953911, 'learning_rate': 1e-06, 'epoch': 0.6}\n",
      "{'loss': 0.3224, 'grad_norm': 12.313382964255357, 'learning_rate': 1e-06, 'epoch': 0.62}\n",
      "{'loss': 0.3998, 'grad_norm': 18.142221411776074, 'learning_rate': 1e-06, 'epoch': 0.63}\n",
      "{'loss': 0.5016, 'grad_norm': 16.746965275752668, 'learning_rate': 1e-06, 'epoch': 0.65}\n",
      "  8%|███                                     | 39/500 [51:25<8:40:59, 67.81s/it]\n",
      " 53%|███████████████████████▍                    | 8/15 [00:39<00:38,  5.50s/it]\u001b[A\n",
      " 60%|██████████████████████████▍                 | 9/15 [00:44<00:33,  5.51s/it]\u001b[A\n",
      " 67%|████████████████████████████▋              | 10/15 [00:50<00:27,  5.56s/it]\u001b[A\n",
      " 73%|███████████████████████████████▌           | 11/15 [00:55<00:22,  5.52s/it]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 12/15 [01:01<00:16,  5.58s/it]\u001b[A\n",
      " 87%|█████████████████████████████████████▎     | 13/15 [01:07<00:11,  5.57s/it]\u001b[A\n",
      " 93%|████████████████████████████████████████▏  | 14/15 [01:12<00:05,  5.56s/it]\u001b[A\n",
      "100%|███████████████████████████████████████████| 15/15 [01:17<00:00,  5.44s/it]\u001b[A\n",
      "{'eval_loss': 0.36720359325408936, 'eval_runtime': 83.5123, 'eval_samples_per_second': 0.359, 'eval_steps_per_second': 0.18, 'epoch': 0.67}\n",
      "\n",
      "  8%|███▏                                    | 40/500 [53:52<8:29:56, 66.51s/it]\u001b[A\n",
      "                                                                                \u001b[A/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "{'loss': 0.4666, 'grad_norm': 15.905080096496734, 'learning_rate': 1e-06, 'epoch': 0.68}\n",
      "{'loss': 0.0959, 'grad_norm': 7.15544716941827, 'learning_rate': 1e-06, 'epoch': 0.7}\n",
      "  8%|███▏                                  | 42/500 [58:29<14:30:27, 114.03s/it]^C\n",
      "[2024-06-12 22:10:10,638] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGINT death signal, shutting down workers\n",
      "[2024-06-12 22:10:10,639] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 573 closing signal SIGINT\n",
      "[2024-06-12 22:10:10,639] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 574 closing signal SIGINT\n"
     ]
    }
   ],
   "source": [
    "!./finetune_ds.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c375ea-c7be-4b0f-a5d1-63d0025bf6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard --logdir /path/to/output/output_minicpmv2 --port 6006\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1c1cc70-7b8c-43b3-8537-b9998040e636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The repository for /workspace/output/output_minicpmv2/checkpoint-40 contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co//workspace/output/output_minicpmv2/checkpoint-40.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n",
      "The repository for /workspace/output/output_minicpmv2/checkpoint-40 contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co//workspace/output/output_minicpmv2/checkpoint-40.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e55012da51d947c495c147a4add9f1fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The repository for /workspace/output/output_minicpmv2/checkpoint-40 contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co//workspace/output/output_minicpmv2/checkpoint-40.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Test the loaded model and tokenizer\u001b[39;00m\n\u001b[1;32m     13\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/workspace/MEng_Project/Synthetic_Image_Annotation/images/1_qsEHMuQeNbIvA4sAI7-pqej55cdSeCP.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     15\u001b[0m response \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mchat(\n\u001b[0;32m---> 16\u001b[0m     image\u001b[38;5;241m=\u001b[39m\u001b[43mImage\u001b[49m\u001b[38;5;241m.\u001b[39mopen(image_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     17\u001b[0m     msgs\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     18\u001b[0m         {\n\u001b[1;32m     19\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     20\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<image>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPlease transcribe only the grade data from this image of a student transcript into csv format.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m         }\n\u001b[1;32m     22\u001b[0m     ],\n\u001b[1;32m     23\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Path to the directory containing the checkpoint\n",
    "model_checkpoint_dir = \"/workspace/output/output_minicpmv2/checkpoint-40\"\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint_dir, use_safetensors=True)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc36c7ec-9957-450a-8a56-e69686583e41",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Test the loaded model and tokenizer\u001b[39;00m\n\u001b[1;32m      3\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/workspace/MEng_Project/Synthetic_Image_Annotation/images/1_qsEHMuQeNbIvA4sAI7-pqej55cdSeCP.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsgs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPlease transcribe only the grade data from this image of a student transcript into csv format.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/openbmb/MiniCPM-Llama3-V-2_5/ee856ed2bc232cfb61d38fa28397be5b48d7944e/modeling_minicpmv.py:454\u001b[0m, in \u001b[0;36mMiniCPMV.chat\u001b[0;34m(self, image, msgs, tokenizer, vision_hidden_states, max_new_tokens, sampling, max_inp_length, system_prompt, stream, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m generation_config\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m    450\u001b[0m     (k, kwargs[k]) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;241m&\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m    451\u001b[0m )\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m--> 454\u001b[0m     res, vision_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_id_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_inp_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_inp_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtgt_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtgt_sizes\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvision_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_vision_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstream_gen\u001b[39m():\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/openbmb/MiniCPM-Llama3-V-2_5/ee856ed2bc232cfb61d38fa28397be5b48d7944e/modeling_minicpmv.py:359\u001b[0m, in \u001b[0;36mMiniCPMV.generate\u001b[0;34m(self, input_id_list, img_list, tgt_sizes, tokenizer, max_inp_length, vision_hidden_states, return_vision_hidden_states, stream, **kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode_stream(model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m], tokenizer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 359\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs_embeds\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_vision_hidden_states:\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result, vision_hidden_states\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/openbmb/MiniCPM-Llama3-V-2_5/ee856ed2bc232cfb61d38fa28397be5b48d7944e/modeling_minicpmv.py:221\u001b[0m, in \u001b[0;36mMiniCPMV._decode\u001b[0;34m(self, inputs_embeds, tokenizer, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decode\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs_embeds, tokenizer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    217\u001b[0m     terminators \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    218\u001b[0m         tokenizer\u001b[38;5;241m.\u001b[39meos_token_id,\n\u001b[1;32m    219\u001b[0m         tokenizer\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|eot_id|>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    220\u001b[0m     ]\n\u001b[0;32m--> 221\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mterminators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode_text(output, tokenizer)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1758\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1750\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1751\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1752\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1753\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1754\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1755\u001b[0m     )\n\u001b[1;32m   1757\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 1758\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1759\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1760\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1761\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1762\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1763\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1764\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1765\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1766\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1767\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1769\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   1770\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1771\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1772\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config) \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1773\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2397\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2394\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2396\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2397\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2398\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2400\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2401\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2402\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2405\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py:1164\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1161\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1163\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1164\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1177\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py:968\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    957\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    958\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    959\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    965\u001b[0m         cache_position,\n\u001b[1;32m    966\u001b[0m     )\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 968\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    978\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py:713\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    710\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    712\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 713\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    722\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    724\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py:355\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(attn_weights, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(query_states\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    354\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(attn_weights, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_dropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[0;32m--> 355\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m (bsz, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim):\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    359\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`attn_output` should be of size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(bsz,\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\u001b[38;5;250m \u001b[39mq_len,\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    360\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattn_output\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    361\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "# Test the loaded model and tokenizer\n",
    "image_path = '/workspace/MEng_Project/Synthetic_Image_Annotation/images/1_qsEHMuQeNbIvA4sAI7-pqej55cdSeCP.jpg'\n",
    "\n",
    "response = model.chat(\n",
    "    image=Image.open(image_path).convert(\"RGB\"),\n",
    "    msgs=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Please transcribe only the grade data from this image of a student transcript into csv format.\"\n",
    "        }\n",
    "    ],\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12b71fa3-1005-4d9d-bb38-ebbbfadd457b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.23.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.9.0)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface_hub)\n",
      "  Downloading fsspec-2024.6.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
      "Collecting tqdm>=4.42.1 (from huggingface_hub)\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2022.12.7)\n",
      "Downloading huggingface_hub-0.23.3-py3-none-any.whl (401 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.7/401.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.6.0-py3-none-any.whl (176 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.9/176.9 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, fsspec, huggingface_hub\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "Successfully installed fsspec-2024.6.0 huggingface_hub-0.23.3 tqdm-4.66.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aea9e423d844967b4e15d83c7e53419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rng_state_0.pth:   0%|          | 0.00/14.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bf118c0bc654f2aa9a9b5a651cf2bb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a56effdb4a0f44619ee670faa3acfc16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/2.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09e75783d224dd199b0c7db3b33b610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aac13808cbf45438138d9739d3145ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea1241ac367c4071883f0090a3535a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 7 LFS files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be6e7e451d4c469b9d5364e7c1d6164b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rng_state_1.pth:   0%|          | 0.00/14.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe5140d0a6e64d1e8a379047ac62914c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/7.61k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected files uploaded successfully!\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub\n",
    "from huggingface_hub import HfApi, upload_folder, create_repo\n",
    "import os\n",
    "\n",
    "# Define your Hugging Face credentials and repository details\n",
    "api_token = \"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\"\n",
    "repo_name = \"DeclanBracken/MiniCPM-Llama3-V-2.5-Transcriptor\"\n",
    "model_dir = \"/workspace/output/output_minicpmv2/checkpoint-40\"\n",
    "\n",
    "# Initialize the HfApi\n",
    "api = HfApi()\n",
    "\n",
    "# Create a new repository if it doesn't exist\n",
    "create_repo(repo_id=repo_name, token=api_token)\n",
    "\n",
    "# Upload the entire model directory\n",
    "upload_folder(\n",
    "    repo_id=repo_name,\n",
    "    folder_path=model_dir,\n",
    "    commit_message=\"Initial commit of fine-tuned model\",\n",
    "    token=api_token\n",
    ")\n",
    "\n",
    "print(\"Selected files uploaded successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
